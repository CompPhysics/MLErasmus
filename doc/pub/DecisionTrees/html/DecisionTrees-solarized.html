<!--
Automatically generated HTML file from DocOnce source
(https://github.com/hplgit/doconce/)
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/hplgit/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Data Analysis and Machine Learning: Trees, forests and all that">

<title>Data Analysis and Machine Learning: Trees, forests and all that</title>


<link href="https://cdn.rawgit.com/hplgit/doconce/master/bundled/html_styles/style_solarized_box/css/solarized_light_code.css" rel="stylesheet" type="text/css" title="light"/>
<script src="https://cdn.rawgit.com/hplgit/doconce/master/bundled/html_styles/style_solarized_box/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<link href="https://thomasf.github.io/solarized-css/solarized-light.min.css" rel="stylesheet">
<style type="text/css">
h1 {color: #b58900;}  /* yellow */
/* h1 {color: #cb4b16;}  orange */
/* h1 {color: #d33682;}  magenta, the original choice of thomasf */
code { padding: 0px; background-color: inherit; }
pre {
  border: 0pt solid #93a1a1;
  box-shadow: none;
}
.alert-text-small   { font-size: 80%;  }
.alert-text-large   { font-size: 130%; }
.alert-text-normal  { font-size: 90%;  }
.alert {
  padding:8px 35px 8px 14px; margin-bottom:18px;
  text-shadow:0 1px 0 rgba(255,255,255,0.5);
  border:1px solid #93a1a1;
  border-radius: 4px;
  -webkit-border-radius: 4px;
  -moz-border-radius: 4px;
  color: #555;
  background-color: #eee8d5;
  background-position: 10px 5px;
  background-repeat: no-repeat;
  background-size: 38px;
  padding-left: 55px;
  width: 75%;
 }
.alert-block {padding-top:14px; padding-bottom:14px}
.alert-block > p, .alert-block > ul {margin-bottom:1em}
.alert li {margin-top: 1em}
.alert-block p+p {margin-top:5px}
.alert-notice { background-image: url(https://cdn.rawgit.com/hplgit/doconce/master/bundled/html_images/small_yellow_notice.png); }
.alert-summary  { background-image:url(https://cdn.rawgit.com/hplgit/doconce/master/bundled/html_images/small_yellow_summary.png); }
.alert-warning { background-image: url(https://cdn.rawgit.com/hplgit/doconce/master/bundled/html_images/small_yellow_warning.png); }
.alert-question {background-image:url(https://cdn.rawgit.com/hplgit/doconce/master/bundled/html_images/small_yellow_question.png); }

div { text-align: justify; text-justify: inter-word; }
</style>


</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Decision trees, overarching aims', 2, None, '___sec0'),
              ('How do we set it up?', 2, None, '___sec1'),
              ('Decision trees and Regression', 2, None, '___sec2'),
              ('Building a tree, regression', 2, None, '___sec3'),
              ('A top-down approach, recursive binary splitting',
               2,
               None,
               '___sec4'),
              ('Making a tree', 2, None, '___sec5'),
              ('Finding the optimal subdivision', 2, None, '___sec6'),
              ('Pruning the tree', 2, None, '___sec7'),
              ('Cost complexity pruning', 2, None, '___sec8'),
              ('A schematic procedure', 2, None, '___sec9'),
              ('A classification tree', 2, None, '___sec10'),
              ('Growing a classification tree', 2, None, '___sec11'),
              ('Classification tree, how to split nodes', 2, None, '___sec12'),
              ('Back to moons again', 2, None, '___sec13'),
              ('Playing around with regions', 2, None, '___sec14'),
              ('Regression trees', 2, None, '___sec15'),
              ('Final regressor code', 2, None, '___sec16'),
              ('Pros and cons of trees, pros', 2, None, '___sec17'),
              ('Disadvantages', 2, None, '___sec18'),
              ('Bagging', 2, None, '___sec19'),
              ('Simple example, head or tail', 2, None, '___sec20'),
              ('Random forests', 2, None, '___sec21'),
              ('A simple scikit-learn example', 2, None, '___sec22'),
              ('Please, not the moons again!', 2, None, '___sec23'),
              ('Bagging examples', 2, None, '___sec24'),
              ('Then random forests', 2, None, '___sec25')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "AMS"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



    
<!-- ------------------- main content ---------------------- -->



<center><h1>Data Analysis and Machine Learning: Trees, forests and all that</h1></center>  <!-- document title -->

<p>
<!-- author(s): Morten Hjorth-Jensen -->

<center>
<b>Morten Hjorth-Jensen</b> [1, 2]
</center>

<p>
<!-- institution(s) -->

<center>[1] <b>Department of Physics, University of Oslo</b></center>
<center>[2] <b>Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University</b></center>
<br>
<p>
<center><h4>Jan 31, 2019</h4></center> <!-- date -->
<br>
<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec0">Decision trees, overarching aims  </h2>

<p>
Decision trees are supervised learning algorithms used for both,
classification and regression tasks.

<p>
The main idea of decision trees
is to find those descriptive features which contain the most
<b>information</b> regarding the target feature and then split the dataset
along the values of these features such that the target feature values
for the resulting sub datasets are as pure as possible.

<p>
The descriptive feature which leaves the target feature most purely is said
to be the most informative one. This process of finding the <b>most
informative</b> feature is done until we accomplish a stopping criteria
where we then finally end up in so called <b>leaf nodes</b>.

<p>
The leaf nodes
contain the predictions we will make for new query instances presented
to our trained model. This is possible since the model has kind of
learned the underlying structure of the training data and hence can,
given some assumptions, make predictions about the target feature value
(class) of unseen query instances.

<p>
A decision tree mainly contains of a <b>root node</b>, <b>interior nodes</b>,
and <b>leaf nodes</b> which are then connected by <b>branches</b>.

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec1">How do we set it up? </h2>

<p>
In simplified terms, the process of training a decision tree and
predicting the target features of query instances is as follows:

<ol>
<li> Present a dataset containing of a number of training instances characterized by a number of descriptive features and a target feature</li>
<li> Train the decision tree model by continuously splitting the target feature along the values of the descriptive features using a measure of information gain during the training process</li>
<li> Grow the tree until we accomplish a stopping criteria create leaf nodes which represent the <em>predictions</em> we want to make for new query instances</li>
<li> Show query instances to the tree and run down the tree until we arrive at leaf nodes</li>
</ol>

Then we are essentially done!

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec2">Decision trees and Regression  </h2>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.preprocessing</span> <span style="color: #8B008B; font-weight: bold">import</span> PolynomialFeatures
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.linear_model</span> <span style="color: #8B008B; font-weight: bold">import</span> LinearRegression

steps=<span style="color: #B452CD">250</span>

distance=<span style="color: #B452CD">0</span>
x=<span style="color: #B452CD">0</span>
distance_list=[]
steps_list=[]
<span style="color: #8B008B; font-weight: bold">while</span> x&lt;steps:
    distance+=np.random.randint(-<span style="color: #B452CD">1</span>,<span style="color: #B452CD">2</span>)
    distance_list.append(distance)
    x+=<span style="color: #B452CD">1</span>
    steps_list.append(x)
plt.plot(steps_list,distance_list, color=<span style="color: #CD5555">&#39;green&#39;</span>, label=<span style="color: #CD5555">&quot;Random Walk Data&quot;</span>)

steps_list=np.asarray(steps_list)
distance_list=np.asarray(distance_list)

X=steps_list[:,np.newaxis]

<span style="color: #228B22">#Polynomial fits</span>

<span style="color: #228B22">#Degree 2</span>
poly_features=PolynomialFeatures(degree=<span style="color: #B452CD">2</span>, include_bias=<span style="color: #658b00">False</span>)
X_poly=poly_features.fit_transform(X)

lin_reg=LinearRegression()
poly_fit=lin_reg.fit(X_poly,distance_list)
b=lin_reg.coef_
c=lin_reg.intercept_
<span style="color: #8B008B; font-weight: bold">print</span> (<span style="color: #CD5555">&quot;2nd degree coefficients:&quot;</span>)
<span style="color: #8B008B; font-weight: bold">print</span> (<span style="color: #CD5555">&quot;zero power: &quot;</span>,c)
<span style="color: #8B008B; font-weight: bold">print</span> (<span style="color: #CD5555">&quot;first power: &quot;</span>, b[<span style="color: #B452CD">0</span>])
<span style="color: #8B008B; font-weight: bold">print</span> (<span style="color: #CD5555">&quot;second power: &quot;</span>,b[<span style="color: #B452CD">1</span>])

z = np.arange(<span style="color: #B452CD">0</span>, steps, .<span style="color: #B452CD">01</span>)
z_mod=b[<span style="color: #B452CD">1</span>]*z**<span style="color: #B452CD">2</span>+b[<span style="color: #B452CD">0</span>]*z+c

fit_mod=b[<span style="color: #B452CD">1</span>]*X**<span style="color: #B452CD">2</span>+b[<span style="color: #B452CD">0</span>]*X+c
plt.plot(z, z_mod, color=<span style="color: #CD5555">&#39;r&#39;</span>, label=<span style="color: #CD5555">&quot;2nd Degree Fit&quot;</span>)
plt.title(<span style="color: #CD5555">&quot;Polynomial Regression&quot;</span>)

plt.xlabel(<span style="color: #CD5555">&quot;Steps&quot;</span>)
plt.ylabel(<span style="color: #CD5555">&quot;Distance&quot;</span>)

<span style="color: #228B22">#Degree 10</span>
poly_features10=PolynomialFeatures(degree=<span style="color: #B452CD">10</span>, include_bias=<span style="color: #658b00">False</span>)
X_poly10=poly_features10.fit_transform(X)

poly_fit10=lin_reg.fit(X_poly10,distance_list)

y_plot=poly_fit10.predict(X_poly10)
plt.plot(X, y_plot, color=<span style="color: #CD5555">&#39;black&#39;</span>, label=<span style="color: #CD5555">&quot;10th Degree Fit&quot;</span>)

plt.legend()
plt.show()


<span style="color: #228B22">#Decision Tree Regression</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.tree</span> <span style="color: #8B008B; font-weight: bold">import</span> DecisionTreeRegressor
regr_1=DecisionTreeRegressor(max_depth=<span style="color: #B452CD">2</span>)
regr_2=DecisionTreeRegressor(max_depth=<span style="color: #B452CD">5</span>)
regr_3=DecisionTreeRegressor(max_depth=<span style="color: #B452CD">7</span>)
regr_1.fit(X, distance_list)
regr_2.fit(X, distance_list)
regr_3.fit(X, distance_list)

X_test = np.arange(<span style="color: #B452CD">0.0</span>, steps, <span style="color: #B452CD">0.01</span>)[:, np.newaxis]
y_1 = regr_1.predict(X_test)
y_2 = regr_2.predict(X_test)
y_3=regr_3.predict(X_test)

<span style="color: #228B22"># Plot the results</span>
plt.figure()
plt.scatter(X, distance_list, s=<span style="color: #B452CD">2.5</span>, c=<span style="color: #CD5555">&quot;black&quot;</span>, label=<span style="color: #CD5555">&quot;data&quot;</span>)
plt.plot(X_test, y_1, color=<span style="color: #CD5555">&quot;red&quot;</span>,
         label=<span style="color: #CD5555">&quot;max_depth=2&quot;</span>, linewidth=<span style="color: #B452CD">2</span>)
plt.plot(X_test, y_2, color=<span style="color: #CD5555">&quot;green&quot;</span>, label=<span style="color: #CD5555">&quot;max_depth=5&quot;</span>, linewidth=<span style="color: #B452CD">2</span>)
plt.plot(X_test, y_3, color=<span style="color: #CD5555">&quot;m&quot;</span>, label=<span style="color: #CD5555">&quot;max_depth=7&quot;</span>, linewidth=<span style="color: #B452CD">2</span>)

plt.xlabel(<span style="color: #CD5555">&quot;Data&quot;</span>)
plt.ylabel(<span style="color: #CD5555">&quot;Darget&quot;</span>)
plt.title(<span style="color: #CD5555">&quot;Decision Tree Regression&quot;</span>)
plt.legend()
plt.show()
</pre></div>
<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec3">Building a tree, regression </h2>

<p>
There are mainly two steps

<ol>
<li> We split the predictor space (the set of possible values \( x_1,x_2,\dots, x_p \)) into \( J \)</li>
</ol>

distinct and non-non-overlapping regions, \( R_1,R_2,\dots,R_J \).  

<ol>
<li> For every observation that falls into the region \( R_j \) , we make the same prediction, which is simply the mean of the response values for the training observations in \( R_j \).</li>
</ol>

How do we construct the regions \( R_1,\dots,R_J \)? 
In theory, the regions could have any shape. However, we
choose to divide the predictor space into high-dimensional rectangles,
or boxes, for simplicity and for ease of interpretation of the
resulting predic- tive model. The goal is to find boxes \( R_1,\dots,R_J \) 
that minimize the MSE, given by 
$$
\sum_{j=1}^J\sum_{i\in R_j}(y_i-\overline{y}_{R_j})^2,
$$

where \( \overline{y}_{R_j} \)  is the mean response for the training observations 
within the $j$th
box.

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec4">A top-down approach, recursive binary splitting </h2>

<p>
Unfortunately, it is computationally infeasible to consider every
possible partition of the feature space into \( J \) boxes. 
The common strategy is to take a top-down approach

<p>
The approach is top-down because it begins at the top of the tree (all
observations belong to a single region) and then successively splits
the predictor space; each split is indicated via two new branches
further down on the tree. It is greedy because at each step of the
tree-building process, the best split is made at that particular step,
rather than looking ahead and picking a split that will lead to a
better tree in some future step.

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec5">Making a tree </h2>

<p>
In order to implement the recursive binary splitting we start by selecting
the predictor \( x_j \) and a cutpoint \( s \) that splits the predictor space into two regions \( R_1 \) and \( R_2 \)
$$
\left\{X\vert x_j < s\right\},
$$

and
$$
\left\{X\vert x_j \geq s\right\},
$$

so that we obtain the lowest MSE, that is
$$
\sum_{i:x_i\in R_j}(y_i-\overline{y}_{R_1})^2+\sum_{i:x_i\in R_2}(y_i-\overline{y}_{R_2})^2,
$$

which we want to minimize by considering all predictors \( x_1,x_2,\dots,x_p \).
We consider also all possible values of \( s \) for each predictor. These values could be determined by randomly assigned numbers or by starting at the midpoint and then proceed till we find an optimal value.

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec6">Finding the optimal subdivision </h2>

<p>
For any \( j \) and \( s \), we define the pair of
half-planes where \( \overline{y}_{R_1} \) is the mean response for the training
observations in \( R_1(j,s) \), and \( \overline{y}_{R_2} \) is the mean response for the
training observations in \( R_2(j,s) \).

<p>
Finding the values of j and s that
minimize the above equation can be done quite quickly, especially when the number
of features \( p \) is not too large.

<p>
Next, we repeat the process, looking
for the best predictor and best cutpoint in order to split the data
further so as to minimize the MSE within each of the resulting
regions. However, this time, instead of splitting the entire predictor
space, we split one of the two previously identified regions. We now
have three regions. Again, we look to split one of these three regions
further, so as to minimize the MSE. The process continues until a
stopping criterion is reached; for instance, we may continue until no
region contains more than five observations.

<p>
<!-- !split  -->

<h2 id="___sec7">Pruning the tree </h2>

<p>
The above procedure is rather straightforward, but leads often to
overfitting and unnecessarily large and complicated trees. The basic
idea is to grow a large tree \( T_0 \) and then prune it back in order to
obtain a subtree. A smaller tree with fewer splits (fewer regions) can
lead to smaller variance and better interpretation at the cost of a
little more bias.

<p>
The so-called Cost complexity pruning algorithm gives us a
way to do just this. Rather than considering every possible subtree,
we consider a sequence of trees indexed by a nonnegative tuning
parameter \( \alpha \).

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec8">Cost complexity pruning </h2>
For each value of \( \alpha \)  there corresponds a subtree \( T \in T_0 \) such that
$$
\sum_{m=1}^{\overline{T}}\sum_{i:x_i\in R_m}(y_i-\overline{y}_{R_m})^2+\alpha\overline{T},
$$

is as small as possible. Here \( \overline{T} \) is 
the number of terminal nodes of the tree \( T \) , \( R_m \) is the
rectangle (i.e. the subset of predictor space)  corresponding to the \( m \)-th terminal node.

<p>
The tuning parameter \( \alpha \) controls a trade-off between the subtree&#8217;s
com- plexity and its fit to the training data. When \( \alpha = 0 \), then the
subtree \( T \) will simply equal \( T_0 \), 
because then the above equation just measures the
training error. 
However, as \( \alpha \) increases, there is a price to pay for
having a tree with many terminal nodes. The above equation will
tend to be minimized for a smaller subtree.

<p>
It turns out that as we increase \( \alpha \) from zero
branches get pruned from the tree in a nested and predictable fashion,
so obtaining the whole sequence of subtrees as a function of \( \alpha \) is
easy. We can select a value of \( \alpha \) using a validation set or using
cross-validation. We then return to the full data set and obtain the
subtree corresponding to \( \alpha \).

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec9">A schematic procedure </h2>

<p>
<div class="alert alert-block alert-block alert-text-normal">
<b>Building a Regression Tree.</b>
<p>

<ol>
<li> Use recursive binary splitting to grow a large tree on the training data, stopping only when each terminal node has fewer than some minimum number of observations.</li>
<li> Apply cost complexity pruning to the large tree in order to obtain a sequence of best subtrees, as a function of \( \alpha \).</li>
<li> Use for example \( K \)-fold cross-validation to choose \( \alpha \). Divide the training observations into \( K \) folds. For each \( k=1,2,\dots,K \) we:</li> 

<ul>
  <li> repeat steps 1 and 2 on all but the \( k \)-th fold of the training data.</li> 
  <li> Then we valuate the mean squared prediction error on the data in the left-out \( k \)-th fold, as a function of \( \alpha \).</li>
  <li> Finally  we average the results for each value of \( \alpha \), and pick \( \alpha \) to minimize the average error.</li>
</ul>

<li> Return the subtree from Step 2 that corresponds to the chosen value of \( \alpha \).</li> 
</ol>
</div>


<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec10">A classification tree </h2>

<p>
A classification tree is very similar to a regression tree, except
that it is used to predict a qualitative response rather than a
quantitative one. Recall that for a regression tree, the predicted
response for an observation is given by the mean response of the
training observations that belong to the same terminal node. In
contrast, for a classification tree, we predict that each observation
belongs to the most commonly occurring class of training observations
in the region to which it belongs. In interpreting the results of a
classification tree, we are often interested not only in the class
prediction corresponding to a particular terminal node region, but
also in the class proportions among the training observations that
fall into that region.

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec11">Growing a classification tree </h2>

<p>
The task of growing a
classification tree is quite similar to the task of growing a
regression tree. Just as in the regression setting, we use recursive
binary splitting to grow a classification tree. However, in the
classification setting, the MSE cannot be used as a criterion for making
the binary splits.  A natural alternative to MSE is the <b>classification
error rate</b>. Since we plan to assign an observation in a given region
to the most commonly occurring error rate class of training
observations in that region, the classification error rate is simply
the fraction of the training observations in that region that do not
belong to the most common class.

<p>
When building a classification tree, either the Gini index or the
entropy are typically used to evaluate the quality of a particular
split, since these two approaches are more sensitive to node purity
than is the classification error rate.

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec12">Classification tree, how to split nodes </h2>
If our targets are the outcome of a classification process that takes for example 
\( k=1,2,\dots,K \) values, the only thing we need to think of is to set up the splitting criteria for each node.

<p>
We define a PDF \( p_{mk} \)  that represents the number of observations of a class \( k \) in a region \( R_m \) with \( N_m \) observations. We represent this likelihood function in terms of the proportion \( I(y_i=k) \)  of observations of this class in the region \( R_m \) as
$$
p_{mk} = \frac{1}{N_m}\sum_{x_i\in R_m}I(y_i=k).
$$

<p>
We let \( p_{mk} \) represent the majority class of observations in region \( m \). The three most common ways of splitting a node are given by 

<ul>
<li> Misclassification error</li> 
</ul>

$$
p_{mk} = \frac{1}{N_m}\sum_{x_i\in R_m}I(y_i\ne k) = 1-p_{mk}.
$$


<ul>
<li> Gini index \( g \)</li>
</ul>

$$
g = \sum_{k=1}^K p_{mk}(1-p_{mk}).
$$


<ul>
<li> Information entropy or just entropy \( s \)</li>
</ul>

$$
s = -\sum_{k=1}^K p_{mk}\log{p_{mk}}.
$$

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec13">Back to moons again </h2>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">__future__</span> <span style="color: #8B008B; font-weight: bold">import</span> division, print_function, unicode_literals

<span style="color: #228B22"># Common imports</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">os</span>

<span style="color: #228B22"># to make this notebook&#39;s output stable across runs</span>
np.random.seed(<span style="color: #B452CD">42</span>)

<span style="color: #228B22"># To plot pretty figures</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">matplotlib.colors</span> <span style="color: #8B008B; font-weight: bold">import</span> ListedColormap
plt.rcParams[<span style="color: #CD5555">&#39;axes.labelsize&#39;</span>] = <span style="color: #B452CD">14</span>
plt.rcParams[<span style="color: #CD5555">&#39;xtick.labelsize&#39;</span>] = <span style="color: #B452CD">12</span>
plt.rcParams[<span style="color: #CD5555">&#39;ytick.labelsize&#39;</span>] = <span style="color: #B452CD">12</span>


<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.svm</span> <span style="color: #8B008B; font-weight: bold">import</span> SVC
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn</span> <span style="color: #8B008B; font-weight: bold">import</span> datasets
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.tree</span> <span style="color: #8B008B; font-weight: bold">import</span> DecisionTreeClassifier
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.datasets</span> <span style="color: #8B008B; font-weight: bold">import</span> make_moons
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.tree</span> <span style="color: #8B008B; font-weight: bold">import</span> export_graphviz

Xm, ym = make_moons(n_samples=<span style="color: #B452CD">100</span>, noise=<span style="color: #B452CD">0.25</span>, random_state=<span style="color: #B452CD">53</span>)

deep_tree_clf1 = DecisionTreeClassifier(random_state=<span style="color: #B452CD">42</span>)
deep_tree_clf2 = DecisionTreeClassifier(min_samples_leaf=<span style="color: #B452CD">4</span>, random_state=<span style="color: #B452CD">42</span>)
deep_tree_clf1.fit(Xm, ym)
deep_tree_clf2.fit(Xm, ym)


<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">plot_decision_boundary</span>(clf, X, y, axes=[<span style="color: #B452CD">0</span>, <span style="color: #B452CD">7.5</span>, <span style="color: #B452CD">0</span>, <span style="color: #B452CD">3</span>], iris=<span style="color: #658b00">True</span>, legend=<span style="color: #658b00">False</span>, plot_training=<span style="color: #658b00">True</span>):
    x1s = np.linspace(axes[<span style="color: #B452CD">0</span>], axes[<span style="color: #B452CD">1</span>], <span style="color: #B452CD">100</span>)
    x2s = np.linspace(axes[<span style="color: #B452CD">2</span>], axes[<span style="color: #B452CD">3</span>], <span style="color: #B452CD">100</span>)
    x1, x2 = np.meshgrid(x1s, x2s)
    X_new = np.c_[x1.ravel(), x2.ravel()]
    y_pred = clf.predict(X_new).reshape(x1.shape)
    custom_cmap = ListedColormap([<span style="color: #CD5555">&#39;#fafab0&#39;</span>,<span style="color: #CD5555">&#39;#9898ff&#39;</span>,<span style="color: #CD5555">&#39;#a0faa0&#39;</span>])
    plt.contourf(x1, x2, y_pred, alpha=<span style="color: #B452CD">0.3</span>, cmap=custom_cmap)
    <span style="color: #8B008B; font-weight: bold">if</span> <span style="color: #8B008B">not</span> iris:
        custom_cmap2 = ListedColormap([<span style="color: #CD5555">&#39;#7d7d58&#39;</span>,<span style="color: #CD5555">&#39;#4c4c7f&#39;</span>,<span style="color: #CD5555">&#39;#507d50&#39;</span>])
        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=<span style="color: #B452CD">0.8</span>)
    <span style="color: #8B008B; font-weight: bold">if</span> plot_training:
        plt.plot(X[:, <span style="color: #B452CD">0</span>][y==<span style="color: #B452CD">0</span>], X[:, <span style="color: #B452CD">1</span>][y==<span style="color: #B452CD">0</span>], <span style="color: #CD5555">&quot;yo&quot;</span>, label=<span style="color: #CD5555">&quot;Iris-Setosa&quot;</span>)
        plt.plot(X[:, <span style="color: #B452CD">0</span>][y==<span style="color: #B452CD">1</span>], X[:, <span style="color: #B452CD">1</span>][y==<span style="color: #B452CD">1</span>], <span style="color: #CD5555">&quot;bs&quot;</span>, label=<span style="color: #CD5555">&quot;Iris-Versicolor&quot;</span>)
        plt.plot(X[:, <span style="color: #B452CD">0</span>][y==<span style="color: #B452CD">2</span>], X[:, <span style="color: #B452CD">1</span>][y==<span style="color: #B452CD">2</span>], <span style="color: #CD5555">&quot;g^&quot;</span>, label=<span style="color: #CD5555">&quot;Iris-Virginica&quot;</span>)
        plt.axis(axes)
    <span style="color: #8B008B; font-weight: bold">if</span> iris:
        plt.xlabel(<span style="color: #CD5555">&quot;Petal length&quot;</span>, fontsize=<span style="color: #B452CD">14</span>)
        plt.ylabel(<span style="color: #CD5555">&quot;Petal width&quot;</span>, fontsize=<span style="color: #B452CD">14</span>)
    <span style="color: #8B008B; font-weight: bold">else</span>:
        plt.xlabel(<span style="color: #CD5555">r&quot;$x_1$&quot;</span>, fontsize=<span style="color: #B452CD">18</span>)
        plt.ylabel(<span style="color: #CD5555">r&quot;$x_2$&quot;</span>, fontsize=<span style="color: #B452CD">18</span>, rotation=<span style="color: #B452CD">0</span>)
    <span style="color: #8B008B; font-weight: bold">if</span> legend:
        plt.legend(loc=<span style="color: #CD5555">&quot;lower right&quot;</span>, fontsize=<span style="color: #B452CD">14</span>)
plt.figure(figsize=(<span style="color: #B452CD">11</span>, <span style="color: #B452CD">4</span>))
plt.subplot(<span style="color: #B452CD">121</span>)
plot_decision_boundary(deep_tree_clf1, Xm, ym, axes=[-<span style="color: #B452CD">1.5</span>, <span style="color: #B452CD">2.5</span>, -<span style="color: #B452CD">1</span>, <span style="color: #B452CD">1.5</span>], iris=<span style="color: #658b00">False</span>)
plt.title(<span style="color: #CD5555">&quot;No restrictions&quot;</span>, fontsize=<span style="color: #B452CD">16</span>)
plt.subplot(<span style="color: #B452CD">122</span>)
plot_decision_boundary(deep_tree_clf2, Xm, ym, axes=[-<span style="color: #B452CD">1.5</span>, <span style="color: #B452CD">2.5</span>, -<span style="color: #B452CD">1</span>, <span style="color: #B452CD">1.5</span>], iris=<span style="color: #658b00">False</span>)
plt.title(<span style="color: #CD5555">&quot;min_samples_leaf = {}&quot;</span>.format(deep_tree_clf2.min_samples_leaf), fontsize=<span style="color: #B452CD">14</span>)
plt.show()
</pre></div>
<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec14">Playing around with regions </h2>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span>np.random.seed(<span style="color: #B452CD">6</span>)
Xs = np.random.rand(<span style="color: #B452CD">100</span>, <span style="color: #B452CD">2</span>) - <span style="color: #B452CD">0.5</span>
ys = (Xs[:, <span style="color: #B452CD">0</span>] &gt; <span style="color: #B452CD">0</span>).astype(np.float32) * <span style="color: #B452CD">2</span>

angle = np.pi / <span style="color: #B452CD">4</span>
rotation_matrix = np.array([[np.cos(angle), -np.sin(angle)], [np.sin(angle), np.cos(angle)]])
Xsr = Xs.dot(rotation_matrix)

tree_clf_s = DecisionTreeClassifier(random_state=<span style="color: #B452CD">42</span>)
tree_clf_s.fit(Xs, ys)
tree_clf_sr = DecisionTreeClassifier(random_state=<span style="color: #B452CD">42</span>)
tree_clf_sr.fit(Xsr, ys)

plt.figure(figsize=(<span style="color: #B452CD">11</span>, <span style="color: #B452CD">4</span>))
plt.subplot(<span style="color: #B452CD">121</span>)
plot_decision_boundary(tree_clf_s, Xs, ys, axes=[-<span style="color: #B452CD">0.7</span>, <span style="color: #B452CD">0.7</span>, -<span style="color: #B452CD">0.7</span>, <span style="color: #B452CD">0.7</span>], iris=<span style="color: #658b00">False</span>)
plt.subplot(<span style="color: #B452CD">122</span>)
plot_decision_boundary(tree_clf_sr, Xsr, ys, axes=[-<span style="color: #B452CD">0.7</span>, <span style="color: #B452CD">0.7</span>, -<span style="color: #B452CD">0.7</span>, <span style="color: #B452CD">0.7</span>], iris=<span style="color: #658b00">False</span>)

plt.show()
</pre></div>
<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec15">Regression trees </h2>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span><span style="color: #228B22"># Quadratic training set + noise</span>
np.random.seed(<span style="color: #B452CD">42</span>)
m = <span style="color: #B452CD">200</span>
X = np.random.rand(m, <span style="color: #B452CD">1</span>)
y = <span style="color: #B452CD">4</span> * (X - <span style="color: #B452CD">0.5</span>) ** <span style="color: #B452CD">2</span>
y = y + np.random.randn(m, <span style="color: #B452CD">1</span>) / <span style="color: #B452CD">10</span>
</pre></div>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.tree</span> <span style="color: #8B008B; font-weight: bold">import</span> DecisionTreeRegressor

tree_reg = DecisionTreeRegressor(max_depth=<span style="color: #B452CD">2</span>, random_state=<span style="color: #B452CD">42</span>)
tree_reg.fit(X, y)
</pre></div>
<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec16">Final regressor code </h2>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.tree</span> <span style="color: #8B008B; font-weight: bold">import</span> DecisionTreeRegressor

tree_reg1 = DecisionTreeRegressor(random_state=<span style="color: #B452CD">42</span>, max_depth=<span style="color: #B452CD">2</span>)
tree_reg2 = DecisionTreeRegressor(random_state=<span style="color: #B452CD">42</span>, max_depth=<span style="color: #B452CD">3</span>)
tree_reg1.fit(X, y)
tree_reg2.fit(X, y)

<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">plot_regression_predictions</span>(tree_reg, X, y, axes=[<span style="color: #B452CD">0</span>, <span style="color: #B452CD">1</span>, -<span style="color: #B452CD">0.2</span>, <span style="color: #B452CD">1</span>], ylabel=<span style="color: #CD5555">&quot;$y$&quot;</span>):
    x1 = np.linspace(axes[<span style="color: #B452CD">0</span>], axes[<span style="color: #B452CD">1</span>], <span style="color: #B452CD">500</span>).reshape(-<span style="color: #B452CD">1</span>, <span style="color: #B452CD">1</span>)
    y_pred = tree_reg.predict(x1)
    plt.axis(axes)
    plt.xlabel(<span style="color: #CD5555">&quot;$x_1$&quot;</span>, fontsize=<span style="color: #B452CD">18</span>)
    <span style="color: #8B008B; font-weight: bold">if</span> ylabel:
        plt.ylabel(ylabel, fontsize=<span style="color: #B452CD">18</span>, rotation=<span style="color: #B452CD">0</span>)
    plt.plot(X, y, <span style="color: #CD5555">&quot;b.&quot;</span>)
    plt.plot(x1, y_pred, <span style="color: #CD5555">&quot;r.-&quot;</span>, linewidth=<span style="color: #B452CD">2</span>, label=<span style="color: #CD5555">r&quot;$\hat{y}$&quot;</span>)

plt.figure(figsize=(<span style="color: #B452CD">11</span>, <span style="color: #B452CD">4</span>))
plt.subplot(<span style="color: #B452CD">121</span>)
plot_regression_predictions(tree_reg1, X, y)
<span style="color: #8B008B; font-weight: bold">for</span> split, style <span style="color: #8B008B">in</span> ((<span style="color: #B452CD">0.1973</span>, <span style="color: #CD5555">&quot;k-&quot;</span>), (<span style="color: #B452CD">0.0917</span>, <span style="color: #CD5555">&quot;k--&quot;</span>), (<span style="color: #B452CD">0.7718</span>, <span style="color: #CD5555">&quot;k--&quot;</span>)):
    plt.plot([split, split], [-<span style="color: #B452CD">0.2</span>, <span style="color: #B452CD">1</span>], style, linewidth=<span style="color: #B452CD">2</span>)
plt.text(<span style="color: #B452CD">0.21</span>, <span style="color: #B452CD">0.65</span>, <span style="color: #CD5555">&quot;Depth=0&quot;</span>, fontsize=<span style="color: #B452CD">15</span>)
plt.text(<span style="color: #B452CD">0.01</span>, <span style="color: #B452CD">0.2</span>, <span style="color: #CD5555">&quot;Depth=1&quot;</span>, fontsize=<span style="color: #B452CD">13</span>)
plt.text(<span style="color: #B452CD">0.65</span>, <span style="color: #B452CD">0.8</span>, <span style="color: #CD5555">&quot;Depth=1&quot;</span>, fontsize=<span style="color: #B452CD">13</span>)
plt.legend(loc=<span style="color: #CD5555">&quot;upper center&quot;</span>, fontsize=<span style="color: #B452CD">18</span>)
plt.title(<span style="color: #CD5555">&quot;max_depth=2&quot;</span>, fontsize=<span style="color: #B452CD">14</span>)

plt.subplot(<span style="color: #B452CD">122</span>)
plot_regression_predictions(tree_reg2, X, y, ylabel=<span style="color: #658b00">None</span>)
<span style="color: #8B008B; font-weight: bold">for</span> split, style <span style="color: #8B008B">in</span> ((<span style="color: #B452CD">0.1973</span>, <span style="color: #CD5555">&quot;k-&quot;</span>), (<span style="color: #B452CD">0.0917</span>, <span style="color: #CD5555">&quot;k--&quot;</span>), (<span style="color: #B452CD">0.7718</span>, <span style="color: #CD5555">&quot;k--&quot;</span>)):
    plt.plot([split, split], [-<span style="color: #B452CD">0.2</span>, <span style="color: #B452CD">1</span>], style, linewidth=<span style="color: #B452CD">2</span>)
<span style="color: #8B008B; font-weight: bold">for</span> split <span style="color: #8B008B">in</span> (<span style="color: #B452CD">0.0458</span>, <span style="color: #B452CD">0.1298</span>, <span style="color: #B452CD">0.2873</span>, <span style="color: #B452CD">0.9040</span>):
    plt.plot([split, split], [-<span style="color: #B452CD">0.2</span>, <span style="color: #B452CD">1</span>], <span style="color: #CD5555">&quot;k:&quot;</span>, linewidth=<span style="color: #B452CD">1</span>)
plt.text(<span style="color: #B452CD">0.3</span>, <span style="color: #B452CD">0.5</span>, <span style="color: #CD5555">&quot;Depth=2&quot;</span>, fontsize=<span style="color: #B452CD">13</span>)
plt.title(<span style="color: #CD5555">&quot;max_depth=3&quot;</span>, fontsize=<span style="color: #B452CD">14</span>)

plt.show()
</pre></div>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span>tree_reg1 = DecisionTreeRegressor(random_state=<span style="color: #B452CD">42</span>)
tree_reg2 = DecisionTreeRegressor(random_state=<span style="color: #B452CD">42</span>, min_samples_leaf=<span style="color: #B452CD">10</span>)
tree_reg1.fit(X, y)
tree_reg2.fit(X, y)

x1 = np.linspace(<span style="color: #B452CD">0</span>, <span style="color: #B452CD">1</span>, <span style="color: #B452CD">500</span>).reshape(-<span style="color: #B452CD">1</span>, <span style="color: #B452CD">1</span>)
y_pred1 = tree_reg1.predict(x1)
y_pred2 = tree_reg2.predict(x1)

plt.figure(figsize=(<span style="color: #B452CD">11</span>, <span style="color: #B452CD">4</span>))

plt.subplot(<span style="color: #B452CD">121</span>)
plt.plot(X, y, <span style="color: #CD5555">&quot;b.&quot;</span>)
plt.plot(x1, y_pred1, <span style="color: #CD5555">&quot;r.-&quot;</span>, linewidth=<span style="color: #B452CD">2</span>, label=<span style="color: #CD5555">r&quot;$\hat{y}$&quot;</span>)
plt.axis([<span style="color: #B452CD">0</span>, <span style="color: #B452CD">1</span>, -<span style="color: #B452CD">0.2</span>, <span style="color: #B452CD">1.1</span>])
plt.xlabel(<span style="color: #CD5555">&quot;$x_1$&quot;</span>, fontsize=<span style="color: #B452CD">18</span>)
plt.ylabel(<span style="color: #CD5555">&quot;$y$&quot;</span>, fontsize=<span style="color: #B452CD">18</span>, rotation=<span style="color: #B452CD">0</span>)
plt.legend(loc=<span style="color: #CD5555">&quot;upper center&quot;</span>, fontsize=<span style="color: #B452CD">18</span>)
plt.title(<span style="color: #CD5555">&quot;No restrictions&quot;</span>, fontsize=<span style="color: #B452CD">14</span>)

plt.subplot(<span style="color: #B452CD">122</span>)
plt.plot(X, y, <span style="color: #CD5555">&quot;b.&quot;</span>)
plt.plot(x1, y_pred2, <span style="color: #CD5555">&quot;r.-&quot;</span>, linewidth=<span style="color: #B452CD">2</span>, label=<span style="color: #CD5555">r&quot;$\hat{y}$&quot;</span>)
plt.axis([<span style="color: #B452CD">0</span>, <span style="color: #B452CD">1</span>, -<span style="color: #B452CD">0.2</span>, <span style="color: #B452CD">1.1</span>])
plt.xlabel(<span style="color: #CD5555">&quot;$x_1$&quot;</span>, fontsize=<span style="color: #B452CD">18</span>)
plt.title(<span style="color: #CD5555">&quot;min_samples_leaf={}&quot;</span>.format(tree_reg2.min_samples_leaf), fontsize=<span style="color: #B452CD">14</span>)

plt.show()
</pre></div>
<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec17">Pros and cons of trees, pros </h2>

<ul>
<li> White box, easy to interpret model. Some people believe that decision trees more closely mirror human decision-making than do the regression and classification approaches discussed earlier (think of support vector machines)</li>
<li> Trees are very easy to explain to people. In fact, they are even easier to explain than linear regression!</li>
<li> No feature normalization needed</li>
<li> Tree models can handle both continuous and categorical data (Classification and Regression Trees)</li>
<li> Can model nonlinear relationships</li>
<li> Can model interactions between the different descriptive features</li>
<li> Trees can be displayed graphically, and are easily interpreted even by a non-expert (especially if they are small)</li>
</ul>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec18">Disadvantages </h2>

<ul>
<li> Unfortunately, trees generally do not have the same level of predictive accuracy as some of the other regression and classification approaches</li>
<li> If continuous features are used the tree may become quite large and hence less interpretable</li>
<li> Decision trees are prone to overfit the training data and hence do not well generalize the data if no stopping criteria or improvements like pruning, boosting or bagging are implemented</li>
<li> Small changes in the data may lead to a completely different tree. This issue can be addressed by using ensemble methods like bagging, boosting or random forests</li>
<li> Unbalanced datasets where some target feature values occur much more frequently than others may lead to biased trees since the frequently occurring feature values are preferred over the less frequently occurring ones.</li> 
<li> If the number of features is relatively large (high dimensional) and the number of instances is relatively low, the tree might overfit the data</li>
<li> Features with many levels may be preferred over features with less levels since for them it is <em>more easy</em> to split the dataset such that the sub datasets only contain pure target feature values. This issue can be addressed by preferring for instance the information gain ratio as splitting criteria over information gain</li>
</ul>

However, by aggregating many decision trees, using methods like bagging, random forests, and boosting, the predictive performance of trees can be substantially improved.

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec19">Bagging </h2>

<p>
The <b>plain</b> decision trees suffer from high
variance. This means that if we split the training data into two parts
at random, and fit a decision tree to both halves, the results that we
get could be quite different. In contrast, a procedure with low
variance will yield similar results if applied repeatedly to distinct
data sets; linear regression tends to have low variance, if the ratio
of \( n \) to \( p \) is moderately large.

<p>
<b>Bootstrap aggregation</b>, or just <b>bagging</b>, is a
general-purpose procedure for reducing the variance of a statistical
learning method.

<p>
Bagging typically results in improved accuracy
over prediction using a single tree. Unfortunately, however, it can be
difficult to interpret the resulting model. Recall that one of the
advantages of decision trees is the attractive and easily interpreted
diagram that results.

<p>
However, when we bag a large number of trees, it is no longer
possible to represent the resulting statistical learning procedure
using a single tree, and it is no longer clear which variables are
most important to the procedure. Thus, bagging improves prediction
accuracy at the expense of interpretability.  Although the collection
of bagged trees is much more difficult to interpret than a single
tree, one can obtain an overall summary of the importance of each
predictor using the MSE (for bagging regression trees) or the Gini
index (for bagging classification trees). In the case of bagging
regression trees, we can record the total amount that the MSE is
decreased due to splits over a given predictor, averaged over all \( B \) possible
trees. A large value indicates an important predictor. Similarly, in
the context of bagging classification trees, we can add up the total
amount that the Gini index  is decreased by splits over a given
predictor, averaged over all \( B \) trees.

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec20">Simple example, head or tail </h2>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span>heads_proba = <span style="color: #B452CD">0.51</span>
coin_tosses = (np.random.rand(<span style="color: #B452CD">10000</span>, <span style="color: #B452CD">10</span>) &lt; heads_proba).astype(np.int32)
cumulative_heads_ratio = np.cumsum(coin_tosses, axis=<span style="color: #B452CD">0</span>) / np.arange(<span style="color: #B452CD">1</span>, <span style="color: #B452CD">10001</span>).reshape(-<span style="color: #B452CD">1</span>, <span style="color: #B452CD">1</span>)
plt.figure(figsize=(<span style="color: #B452CD">8</span>,<span style="color: #B452CD">3.5</span>))
plt.plot(cumulative_heads_ratio)
plt.plot([<span style="color: #B452CD">0</span>, <span style="color: #B452CD">10000</span>], [<span style="color: #B452CD">0.51</span>, <span style="color: #B452CD">0.51</span>], <span style="color: #CD5555">&quot;k--&quot;</span>, linewidth=<span style="color: #B452CD">2</span>, label=<span style="color: #CD5555">&quot;51%&quot;</span>)
plt.plot([<span style="color: #B452CD">0</span>, <span style="color: #B452CD">10000</span>], [<span style="color: #B452CD">0.5</span>, <span style="color: #B452CD">0.5</span>], <span style="color: #CD5555">&quot;k-&quot;</span>, label=<span style="color: #CD5555">&quot;50%&quot;</span>)
plt.xlabel(<span style="color: #CD5555">&quot;Number of coin tosses&quot;</span>)
plt.ylabel(<span style="color: #CD5555">&quot;Heads ratio&quot;</span>)
plt.legend(loc=<span style="color: #CD5555">&quot;lower right&quot;</span>)
plt.axis([<span style="color: #B452CD">0</span>, <span style="color: #B452CD">10000</span>, <span style="color: #B452CD">0.42</span>, <span style="color: #B452CD">0.58</span>])
plt.show()
</pre></div>
<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec21">Random forests </h2>

<p>
Random forests provide an improvement over bagged trees by way of a
small tweak that decorrelates the trees.

<p>
As in bagging, we build a
number of decision trees on bootstrapped training samples. But when
building these decision trees, each time a split in a tree is
considered, a random sample of \( m \) predictors is chosen as split
candidates from the full set of \( p \) predictors. The split is allowed to
use only one of those \( m \) predictors.

<p>
A fresh sample of \( m \) predictors is
taken at each split, and typically we choose 
$$
m\approx \sqrt{p}.
$$

In building a random forest, at
each split in the tree, the algorithm is not even allowed to consider
a majority of the available predictors.

<p>
The reason for this is rather clever. Suppose that there is one very
strong predictor in the data set, along with a number of other
moderately strong predictors. Then in the collection of bagged
variable importance random forest trees, most or all of the trees will
use this strong predictor in the top split. Consequently, all of the
bagged trees will look quite similar to each other. Hence the
predictions from the bagged trees will be highly correlated. 
Unfortunately, averaging many highly correlated quantities does not lead
to as large of a reduction in variance as averaging many uncorrelated
quanti- ties. In particular, this means that bagging will not lead to
a substantial reduction in variance over a single tree in this
setting.

<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec22">A simple scikit-learn example </h2>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.ensemble</span> <span style="color: #8B008B; font-weight: bold">import</span> RandomForestClassifier
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.preprocessing</span> <span style="color: #8B008B; font-weight: bold">import</span> LabelEncoder
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.model_selection</span> <span style="color: #8B008B; font-weight: bold">import</span> cross_validate
<span style="color: #228B22"># Data set not specificied</span>
X = dataset.XXX
Y = dataset.YYY
<span style="color: #228B22">#Instantiate the model with 100 trees and entropy as splitting criteria</span>
Random_Forest_model = RandomForestClassifier(n_estimators=<span style="color: #B452CD">100</span>,criterion=<span style="color: #CD5555">&quot;entropy&quot;</span>)
<span style="color: #228B22">#Cross validation</span>
accuracy = cross_validate(Random_Forest_model,X,Y,cv=<span style="color: #B452CD">10</span>)[<span style="color: #CD5555">&#39;test_score&#39;</span>]
</pre></div>
<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec23">Please, not the moons again! </h2>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.model_selection</span> <span style="color: #8B008B; font-weight: bold">import</span> train_test_split
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.datasets</span> <span style="color: #8B008B; font-weight: bold">import</span> make_moons

X, y = make_moons(n_samples=<span style="color: #B452CD">500</span>, noise=<span style="color: #B452CD">0.30</span>, random_state=<span style="color: #B452CD">42</span>)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span style="color: #B452CD">42</span>)
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.ensemble</span> <span style="color: #8B008B; font-weight: bold">import</span> RandomForestClassifier
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.ensemble</span> <span style="color: #8B008B; font-weight: bold">import</span> VotingClassifier
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.linear_model</span> <span style="color: #8B008B; font-weight: bold">import</span> LogisticRegression
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.svm</span> <span style="color: #8B008B; font-weight: bold">import</span> SVC

log_clf = LogisticRegression(random_state=<span style="color: #B452CD">42</span>)
rnd_clf = RandomForestClassifier(random_state=<span style="color: #B452CD">42</span>)
svm_clf = SVC(random_state=<span style="color: #B452CD">42</span>)

voting_clf = VotingClassifier(
    estimators=[(<span style="color: #CD5555">&#39;lr&#39;</span>, log_clf), (<span style="color: #CD5555">&#39;rf&#39;</span>, rnd_clf), (<span style="color: #CD5555">&#39;svc&#39;</span>, svm_clf)],
    voting=<span style="color: #CD5555">&#39;hard&#39;</span>)
voting_clf.fit(X_train, y_train)
</pre></div>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.metrics</span> <span style="color: #8B008B; font-weight: bold">import</span> accuracy_score

<span style="color: #8B008B; font-weight: bold">for</span> clf <span style="color: #8B008B">in</span> (log_clf, rnd_clf, svm_clf, voting_clf):
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    <span style="color: #8B008B; font-weight: bold">print</span>(clf.<span style="color: #00688B">__class__</span>.<span style="color: #00688B">__name__</span>, accuracy_score(y_test, y_pred))
</pre></div>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span>log_clf = LogisticRegression(random_state=<span style="color: #B452CD">42</span>)
rnd_clf = RandomForestClassifier(random_state=<span style="color: #B452CD">42</span>)
svm_clf = SVC(probability=<span style="color: #658b00">True</span>, random_state=<span style="color: #B452CD">42</span>)

voting_clf = VotingClassifier(
    estimators=[(<span style="color: #CD5555">&#39;lr&#39;</span>, log_clf), (<span style="color: #CD5555">&#39;rf&#39;</span>, rnd_clf), (<span style="color: #CD5555">&#39;svc&#39;</span>, svm_clf)],
    voting=<span style="color: #CD5555">&#39;soft&#39;</span>)
voting_clf.fit(X_train, y_train)
</pre></div>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.metrics</span> <span style="color: #8B008B; font-weight: bold">import</span> accuracy_score

<span style="color: #8B008B; font-weight: bold">for</span> clf <span style="color: #8B008B">in</span> (log_clf, rnd_clf, svm_clf, voting_clf):
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    <span style="color: #8B008B; font-weight: bold">print</span>(clf.<span style="color: #00688B">__class__</span>.<span style="color: #00688B">__name__</span>, accuracy_score(y_test, y_pred))
</pre></div>
<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec24">Bagging examples </h2>

<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.ensemble</span> <span style="color: #8B008B; font-weight: bold">import</span> BaggingClassifier
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.tree</span> <span style="color: #8B008B; font-weight: bold">import</span> DecisionTreeClassifier

bag_clf = BaggingClassifier(
    DecisionTreeClassifier(random_state=<span style="color: #B452CD">42</span>), n_estimators=<span style="color: #B452CD">500</span>,
    max_samples=<span style="color: #B452CD">100</span>, bootstrap=<span style="color: #658b00">True</span>, n_jobs=-<span style="color: #B452CD">1</span>, random_state=<span style="color: #B452CD">42</span>)
bag_clf.fit(X_train, y_train)
y_pred = bag_clf.predict(X_test)
</pre></div>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.metrics</span> <span style="color: #8B008B; font-weight: bold">import</span> accuracy_score
<span style="color: #8B008B; font-weight: bold">print</span>(accuracy_score(y_test, y_pred))
</pre></div>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span>tree_clf = DecisionTreeClassifier(random_state=<span style="color: #B452CD">42</span>)
tree_clf.fit(X_train, y_train)
y_pred_tree = tree_clf.predict(X_test)
<span style="color: #8B008B; font-weight: bold">print</span>(accuracy_score(y_test, y_pred_tree))
</pre></div>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">matplotlib.colors</span> <span style="color: #8B008B; font-weight: bold">import</span> ListedColormap

<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">plot_decision_boundary</span>(clf, X, y, axes=[-<span style="color: #B452CD">1.5</span>, <span style="color: #B452CD">2.5</span>, -<span style="color: #B452CD">1</span>, <span style="color: #B452CD">1.5</span>], alpha=<span style="color: #B452CD">0.5</span>, contour=<span style="color: #658b00">True</span>):
    x1s = np.linspace(axes[<span style="color: #B452CD">0</span>], axes[<span style="color: #B452CD">1</span>], <span style="color: #B452CD">100</span>)
    x2s = np.linspace(axes[<span style="color: #B452CD">2</span>], axes[<span style="color: #B452CD">3</span>], <span style="color: #B452CD">100</span>)
    x1, x2 = np.meshgrid(x1s, x2s)
    X_new = np.c_[x1.ravel(), x2.ravel()]
    y_pred = clf.predict(X_new).reshape(x1.shape)
    custom_cmap = ListedColormap([<span style="color: #CD5555">&#39;#fafab0&#39;</span>,<span style="color: #CD5555">&#39;#9898ff&#39;</span>,<span style="color: #CD5555">&#39;#a0faa0&#39;</span>])
    plt.contourf(x1, x2, y_pred, alpha=<span style="color: #B452CD">0.3</span>, cmap=custom_cmap)
    <span style="color: #8B008B; font-weight: bold">if</span> contour:
        custom_cmap2 = ListedColormap([<span style="color: #CD5555">&#39;#7d7d58&#39;</span>,<span style="color: #CD5555">&#39;#4c4c7f&#39;</span>,<span style="color: #CD5555">&#39;#507d50&#39;</span>])
        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=<span style="color: #B452CD">0.8</span>)
    plt.plot(X[:, <span style="color: #B452CD">0</span>][y==<span style="color: #B452CD">0</span>], X[:, <span style="color: #B452CD">1</span>][y==<span style="color: #B452CD">0</span>], <span style="color: #CD5555">&quot;yo&quot;</span>, alpha=alpha)
    plt.plot(X[:, <span style="color: #B452CD">0</span>][y==<span style="color: #B452CD">1</span>], X[:, <span style="color: #B452CD">1</span>][y==<span style="color: #B452CD">1</span>], <span style="color: #CD5555">&quot;bs&quot;</span>, alpha=alpha)
    plt.axis(axes)
    plt.xlabel(<span style="color: #CD5555">r&quot;$x_1$&quot;</span>, fontsize=<span style="color: #B452CD">18</span>)
    plt.ylabel(<span style="color: #CD5555">r&quot;$x_2$&quot;</span>, fontsize=<span style="color: #B452CD">18</span>, rotation=<span style="color: #B452CD">0</span>)
plt.figure(figsize=(<span style="color: #B452CD">11</span>,<span style="color: #B452CD">4</span>))
plt.subplot(<span style="color: #B452CD">121</span>)
plot_decision_boundary(tree_clf, X, y)
plt.title(<span style="color: #CD5555">&quot;Decision Tree&quot;</span>, fontsize=<span style="color: #B452CD">14</span>)
plt.subplot(<span style="color: #B452CD">122</span>)
plot_decision_boundary(bag_clf, X, y)
plt.title(<span style="color: #CD5555">&quot;Decision Trees with Bagging&quot;</span>, fontsize=<span style="color: #B452CD">14</span>)
plt.show()
</pre></div>
<p>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>

<h2 id="___sec25">Then random forests </h2>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span>bag_clf = BaggingClassifier(
    DecisionTreeClassifier(splitter=<span style="color: #CD5555">&quot;random&quot;</span>, max_leaf_nodes=<span style="color: #B452CD">16</span>, random_state=<span style="color: #B452CD">42</span>),
    n_estimators=<span style="color: #B452CD">500</span>, max_samples=<span style="color: #B452CD">1.0</span>, bootstrap=<span style="color: #658b00">True</span>, n_jobs=-<span style="color: #B452CD">1</span>, random_state=<span style="color: #B452CD">42</span>)
</pre></div>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span></span>bag_clf.fit(X_train, y_train)
y_pred = bag_clf.predict(X_test)
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.ensemble</span> <span style="color: #8B008B; font-weight: bold">import</span> RandomForestClassifier
rnd_clf = RandomForestClassifier(n_estimators=<span style="color: #B452CD">500</span>, max_leaf_nodes=<span style="color: #B452CD">16</span>, n_jobs=-<span style="color: #B452CD">1</span>, random_state=<span style="color: #B452CD">42</span>)
rnd_clf.fit(X_train, y_train)
y_pred_rf = rnd_clf.predict(X_test)
np.sum(y_pred == y_pred_rf) / <span style="color: #658b00">len</span>(y_pred) 
</pre></div>
<p>

<!-- ------------------- end of main content --------------- -->


<center style="font-size:80%">
<!-- copyright --> &copy; 1999-2019, Morten Hjorth-Jensen. Released under CC Attribution-NonCommercial 4.0 license
</center>


</body>
</html>
    

