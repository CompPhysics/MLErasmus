<!--
Automatically generated HTML file from DocOnce source
(https://github.com/hplgit/doconce/)
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/hplgit/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Data Analysis and Machine Learning: Neural networks, from the simple perceptron to deep learning">

<title>Data Analysis and Machine Learning: Neural networks, from the simple perceptron to deep learning</title>

<!-- Bootstrap style: bootstrap -->
<link href="https://netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css" rel="stylesheet">
<!-- not necessary
<link href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
-->

<style type="text/css">

/* Add scrollbar to dropdown menus in bootstrap navigation bar */
.dropdown-menu {
   height: auto;
   max-height: 400px;
   overflow-x: hidden;
}

/* Adds an invisible element before each target to offset for the navigation
   bar */
.anchor::before {
  content:"";
  display:block;
  height:50px;      /* fixed header height for style bootstrap */
  margin:-50px 0 0; /* negative fixed header height */
}
</style>


</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Neural networks', 2, None, '___sec0'),
              ('Artificial neurons', 2, None, '___sec1'),
              ('Neural network types', 2, None, '___sec2'),
              ('Feed-forward neural networks', 2, None, '___sec3'),
              ('Convolutional Neural Network', 2, None, '___sec4'),
              ('Recurrent neural networks', 2, None, '___sec5'),
              ('Other types of networks', 2, None, '___sec6'),
              ('Multilayer perceptrons', 2, None, '___sec7'),
              ('Why multilayer perceptrons?', 2, None, '___sec8'),
              ('Mathematical model', 2, None, '___sec9'),
              ('Mathematical model', 2, None, '___sec10'),
              ('Mathematical model', 2, None, '___sec11'),
              ('Mathematical model', 2, None, '___sec12'),
              ('Mathematical model', 2, None, '___sec13'),
              ('Matrix-vector notation', 3, None, '___sec14'),
              ('Matrix-vector notation  and activation', 3, None, '___sec15'),
              ('Activation functions', 3, None, '___sec16'),
              ('Activation functions, Logistic and Hyperbolic ones',
               3,
               None,
               '___sec17'),
              ('Relevance', 3, None, '___sec18'),
              ('The multilayer  perceptron (MLP)', 2, None, '___sec19'),
              ('From one to many layers, the universal approximation theorem',
               2,
               None,
               '___sec20'),
              ('Deriving the back propagation code for a multilayer perceptron '
               'model',
               2,
               None,
               '___sec21'),
              ('Definitions', 2, None, '___sec22'),
              ('Derivatives and the chain rule', 2, None, '___sec23'),
              ('Derivative of the cost function', 2, None, '___sec24'),
              ('Bringing it together, first back propagation equation',
               2,
               None,
               '___sec25'),
              ('Derivatives in terms of $z_j^L$', 2, None, '___sec26'),
              ('Bringing it together', 2, None, '___sec27'),
              ('Final back propagating equation', 2, None, '___sec28'),
              ('Setting up the Back propagation algorithm',
               2,
               None,
               '___sec29'),
              ('Setting up a Multi-layer perceptron model for classification',
               2,
               None,
               '___sec30'),
              ('Defining the cost function', 2, None, '___sec31'),
              ('Example: binary classification problem', 2, None, '___sec32'),
              ('The Softmax function', 2, None, '___sec33'),
              ('Developing a code for doing neural networks with back '
               'propagation',
               2,
               None,
               '___sec34'),
              ('Collect and pre-process data', 2, None, '___sec35'),
              ('Train and test datasets', 2, None, '___sec36'),
              ('Define model and architecture', 2, None, '___sec37'),
              ('Layers', 2, None, '___sec38'),
              ('Weights and biases', 2, None, '___sec39'),
              ('Feed-forward pass', 2, None, '___sec40'),
              ('Matrix multiplications', 2, None, '___sec41'),
              ('Choose cost function and optimizer', 2, None, '___sec42'),
              ('Optimizing the cost function', 2, None, '___sec43'),
              ('Regularization', 2, None, '___sec44'),
              ('Matrix  multiplication', 2, None, '___sec45'),
              ('Improving performance', 2, None, '___sec46'),
              ('Full object-oriented implementation', 2, None, '___sec47'),
              ('Evaluate model performance on test data', 2, None, '___sec48'),
              ('Adjust hyperparameters', 2, None, '___sec49'),
              ('Visualization', 2, None, '___sec50'),
              ('scikit-learn implementation', 2, None, '___sec51'),
              ('Visualization', 2, None, '___sec52'),
              ('Building neural networks in Tensorflow and Keras',
               2,
               None,
               '___sec53'),
              ('Tensorflow', 2, None, '___sec54'),
              ('Collect and pre-process data', 2, None, '___sec55'),
              ('Using TensorFlow backend', 2, None, '___sec56'),
              ('Optimizing and using gradient descent', 2, None, '___sec57'),
              ('Using Keras', 2, None, '___sec58'),
              ('Which activation function should I use?', 2, None, '___sec59'),
              ('Is the Logistic activation function (Sigmoid)  our choice?',
               2,
               None,
               '___sec60'),
              ('The derivative of the Logistic funtion', 2, None, '___sec61'),
              ('The RELU function family', 2, None, '___sec62'),
              ('Which activation function should we use?', 2, None, '___sec63'),
              ('A top-down perspective on Neural networks',
               2,
               None,
               '___sec64'),
              ('Limitations of supervised learning with deep networks',
               2,
               None,
               '___sec65')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "none"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



    
<!-- Bootstrap navigation bar -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-responsive-collapse">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="NeuralNet-bs.html">Data Analysis and Machine Learning: Neural networks, from the simple perceptron to deep learning</a>
  </div>

  <div class="navbar-collapse collapse navbar-responsive-collapse">
    <ul class="nav navbar-nav navbar-right">
      <li class="dropdown">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Contents <b class="caret"></b></a>
        <ul class="dropdown-menu">
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs001.html#___sec0" style="font-size: 80%;"><b>Neural networks</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs002.html#___sec1" style="font-size: 80%;"><b>Artificial neurons</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs003.html#___sec2" style="font-size: 80%;"><b>Neural network types</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs004.html#___sec3" style="font-size: 80%;"><b>Feed-forward neural networks</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs005.html#___sec4" style="font-size: 80%;"><b>Convolutional Neural Network</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs006.html#___sec5" style="font-size: 80%;"><b>Recurrent neural networks</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs007.html#___sec6" style="font-size: 80%;"><b>Other types of networks</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs008.html#___sec7" style="font-size: 80%;"><b>Multilayer perceptrons</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs009.html#___sec8" style="font-size: 80%;"><b>Why multilayer perceptrons?</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs010.html#___sec9" style="font-size: 80%;"><b>Mathematical model</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs011.html#___sec10" style="font-size: 80%;"><b>Mathematical model</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs012.html#___sec11" style="font-size: 80%;"><b>Mathematical model</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs013.html#___sec12" style="font-size: 80%;"><b>Mathematical model</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs014.html#___sec13" style="font-size: 80%;"><b>Mathematical model</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs015.html#___sec14" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Matrix-vector notation</a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs016.html#___sec15" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Matrix-vector notation  and activation</a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs017.html#___sec16" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Activation functions</a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs018.html#___sec17" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Activation functions, Logistic and Hyperbolic ones</a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs019.html#___sec18" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Relevance</a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs020.html#___sec19" style="font-size: 80%;"><b>The multilayer  perceptron (MLP)</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs021.html#___sec20" style="font-size: 80%;"><b>From one to many layers, the universal approximation theorem</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs022.html#___sec21" style="font-size: 80%;"><b>Deriving the back propagation code for a multilayer perceptron model</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs023.html#___sec22" style="font-size: 80%;"><b>Definitions</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs024.html#___sec23" style="font-size: 80%;"><b>Derivatives and the chain rule</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs025.html#___sec24" style="font-size: 80%;"><b>Derivative of the cost function</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs026.html#___sec25" style="font-size: 80%;"><b>Bringing it together, first back propagation equation</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs027.html#___sec26" style="font-size: 80%;"><b>Derivatives in terms of \( z_j^L \)</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs028.html#___sec27" style="font-size: 80%;"><b>Bringing it together</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs029.html#___sec28" style="font-size: 80%;"><b>Final back propagating equation</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs030.html#___sec29" style="font-size: 80%;"><b>Setting up the Back propagation algorithm</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs031.html#___sec30" style="font-size: 80%;"><b>Setting up a Multi-layer perceptron model for classification</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs032.html#___sec31" style="font-size: 80%;"><b>Defining the cost function</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs033.html#___sec32" style="font-size: 80%;"><b>Example: binary classification problem</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs034.html#___sec33" style="font-size: 80%;"><b>The Softmax function</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs035.html#___sec34" style="font-size: 80%;"><b>Developing a code for doing neural networks with back propagation</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs036.html#___sec35" style="font-size: 80%;"><b>Collect and pre-process data</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs037.html#___sec36" style="font-size: 80%;"><b>Train and test datasets</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs038.html#___sec37" style="font-size: 80%;"><b>Define model and architecture</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs039.html#___sec38" style="font-size: 80%;"><b>Layers</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs040.html#___sec39" style="font-size: 80%;"><b>Weights and biases</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs041.html#___sec40" style="font-size: 80%;"><b>Feed-forward pass</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs042.html#___sec41" style="font-size: 80%;"><b>Matrix multiplications</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs043.html#___sec42" style="font-size: 80%;"><b>Choose cost function and optimizer</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs044.html#___sec43" style="font-size: 80%;"><b>Optimizing the cost function</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs045.html#___sec44" style="font-size: 80%;"><b>Regularization</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs046.html#___sec45" style="font-size: 80%;"><b>Matrix  multiplication</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs047.html#___sec46" style="font-size: 80%;"><b>Improving performance</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs048.html#___sec47" style="font-size: 80%;"><b>Full object-oriented implementation</b></a></li>
     <!-- navigation toc: --> <li><a href="#___sec48" style="font-size: 80%;"><b>Evaluate model performance on test data</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs050.html#___sec49" style="font-size: 80%;"><b>Adjust hyperparameters</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs051.html#___sec50" style="font-size: 80%;"><b>Visualization</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs052.html#___sec51" style="font-size: 80%;"><b>scikit-learn implementation</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs053.html#___sec52" style="font-size: 80%;"><b>Visualization</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs054.html#___sec53" style="font-size: 80%;"><b>Building neural networks in Tensorflow and Keras</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs055.html#___sec54" style="font-size: 80%;"><b>Tensorflow</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs056.html#___sec55" style="font-size: 80%;"><b>Collect and pre-process data</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs057.html#___sec56" style="font-size: 80%;"><b>Using TensorFlow backend</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs058.html#___sec57" style="font-size: 80%;"><b>Optimizing and using gradient descent</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs059.html#___sec58" style="font-size: 80%;"><b>Using Keras</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs060.html#___sec59" style="font-size: 80%;"><b>Which activation function should I use?</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs061.html#___sec60" style="font-size: 80%;"><b>Is the Logistic activation function (Sigmoid)  our choice?</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs062.html#___sec61" style="font-size: 80%;"><b>The derivative of the Logistic funtion</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs063.html#___sec62" style="font-size: 80%;"><b>The RELU function family</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs064.html#___sec63" style="font-size: 80%;"><b>Which activation function should we use?</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs065.html#___sec64" style="font-size: 80%;"><b>A top-down perspective on Neural networks</b></a></li>
     <!-- navigation toc: --> <li><a href="._NeuralNet-bs066.html#___sec65" style="font-size: 80%;"><b>Limitations of supervised learning with deep networks</b></a></li>

        </ul>
      </li>
    </ul>
  </div>
</div>
</div> <!-- end of navigation bar -->

<div class="container">

<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p> <!-- add vertical space -->

<a name="part0049"></a>
<!-- !split -->

<h2 id="___sec48" class="anchor">Evaluate model performance on test data </h2>

<p>
To measure the performance of our network we evaluate how well it does it data it has never seen before, i.e. the test data.  
We measure the performance of the network using the <em>accuracy</em> score.  
The accuracy is as you would expect just the number of images correctly labeled divided by the total number of images. A perfect classifier will have an accuracy score of \( 1 \).  

$$ \text{Accuracy} = \frac{\sum_{i=1}^n I(\hat{y}_i = y_i)}{n} ,$$

<p>
where \( I \) is the indicator function, \( 1 \) if \( \hat{y}_i = y_i \) and \( 0 \) otherwise.

<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span>epochs <span style="color: #666666">=</span> <span style="color: #666666">100</span>
batch_size <span style="color: #666666">=</span> <span style="color: #666666">100</span>

dnn <span style="color: #666666">=</span> NeuralNetwork(X_train, Y_train_onehot, eta<span style="color: #666666">=</span>eta, lmbd<span style="color: #666666">=</span>lmbd, epochs<span style="color: #666666">=</span>epochs, batch_size<span style="color: #666666">=</span>batch_size,
                    n_hidden_neurons<span style="color: #666666">=</span>n_hidden_neurons, n_categories<span style="color: #666666">=</span>n_categories)
dnn<span style="color: #666666">.</span>train()
test_predict <span style="color: #666666">=</span> dnn<span style="color: #666666">.</span>predict(X_test)

<span style="color: #408080; font-style: italic"># accuracy score from scikit library</span>
<span style="color: #008000; font-weight: bold">print</span>(<span style="color: #BA2121">&quot;Accuracy score on test set: &quot;</span>, accuracy_score(Y_test, test_predict))

<span style="color: #408080; font-style: italic"># equivalent in numpy</span>
<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">accuracy_score_numpy</span>(Y_test, Y_pred):
    <span style="color: #008000; font-weight: bold">return</span> np<span style="color: #666666">.</span>sum(Y_test <span style="color: #666666">==</span> Y_pred) <span style="color: #666666">/</span> <span style="color: #008000">len</span>(Y_test)

<span style="color: #408080; font-style: italic">#print(&quot;Accuracy score on test set: &quot;, accuracy_score_numpy(Y_test, test_predict))</span>
</pre></div>
<p>
<p>
<!-- navigation buttons at the bottom of the page -->
<ul class="pagination">
<li><a href="._NeuralNet-bs048.html">&laquo;</a></li>
  <li><a href="._NeuralNet-bs000.html">1</a></li>
  <li><a href="">...</a></li>
  <li><a href="._NeuralNet-bs041.html">42</a></li>
  <li><a href="._NeuralNet-bs042.html">43</a></li>
  <li><a href="._NeuralNet-bs043.html">44</a></li>
  <li><a href="._NeuralNet-bs044.html">45</a></li>
  <li><a href="._NeuralNet-bs045.html">46</a></li>
  <li><a href="._NeuralNet-bs046.html">47</a></li>
  <li><a href="._NeuralNet-bs047.html">48</a></li>
  <li><a href="._NeuralNet-bs048.html">49</a></li>
  <li class="active"><a href="._NeuralNet-bs049.html">50</a></li>
  <li><a href="._NeuralNet-bs050.html">51</a></li>
  <li><a href="._NeuralNet-bs051.html">52</a></li>
  <li><a href="._NeuralNet-bs052.html">53</a></li>
  <li><a href="._NeuralNet-bs053.html">54</a></li>
  <li><a href="._NeuralNet-bs054.html">55</a></li>
  <li><a href="._NeuralNet-bs055.html">56</a></li>
  <li><a href="._NeuralNet-bs056.html">57</a></li>
  <li><a href="._NeuralNet-bs057.html">58</a></li>
  <li><a href="._NeuralNet-bs058.html">59</a></li>
  <li><a href="">...</a></li>
  <li><a href="._NeuralNet-bs066.html">67</a></li>
  <li><a href="._NeuralNet-bs050.html">&raquo;</a></li>
</ul>
<!-- ------------------- end of main content --------------- -->

</div>  <!-- end container -->
<!-- include javascript, jQuery *first* -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>

<!-- Bootstrap footer
<footer>
<a href="http://..."><img width="250" align=right src="http://..."></a>
</footer>
-->


<center style="font-size:80%">
<!-- copyright only on the titlepage -->
</center>


</body>
</html>
    

