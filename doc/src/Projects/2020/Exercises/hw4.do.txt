TITLE: Homework 2
AUTHOR: "Data Analysis and Machine Learning FYS-STK3155/FYS4155":"http://www.uio.no/studier/emner/matnat/fys/FYS3155/index-eng.html" {copyright, 1999-present|CC BY-NC} at Department of Physics, University of Oslo, Norway
DATE:Today


===== Exercise 4  =====

This exercise is a continuation of exercise 2 from homework 1. We will
use the same function to generate our data set, still staying with a
simple function $y(x)$ which we want to fit using linear regression,
but now extending the analysis to include the Ridge and the Lasso
regression methods. You can use the code under the Regression as an example on how to use the Ridge and the Lasso methods, see the "regression slides":"https://compphysics.github.io/MachineLearning/doc/pub/Regression/html/Regression-bs.html"). 

We will thus again generate our own dataset for a function $y(x)$ where 
$x \in [0,1]$ and defined by random numbers computed with the uniform
distribution. The function $y$ is a quadratic polynomial in $x$ with
added stochastic noise according to the normal distribution $\cal{N}(0,1)$.

The following simple Python instructions define our $x$ and $y$ values (with 100 data points).
!bc pycod
x = np.random.rand(100,1)
y = 5*x*x+0.1*np.random.randn(100,1)
!ec

o Write your own code for the Ridge method (see chapter 3.4 of Hastie *et al.*, equations (3.43) and (3.44)) and compute the parametrization for different values of $\lambda$. Compare and analyze your results with those from exercise 2. Study the dependence on $\lambda$ while also varying the strength of the noise in your expression for $y(x)$. 

o Repeat the above but using the functionality of _Scikit-Learn_. Compare your code with the results from _Scikit-Learn_. Remember to run with the same random numbers for generating $x$ and $y$. 

o Our next step is to study the variance of the parameters $\beta_1$ and $\beta_2$ (assuming that we are parameterizing our function with a second-order polynomial). We will use standard linear regression and the Ridge regression.  You can now opt for either writing your own function or using _Scikit-Learn_ to find the parameters $\beta$. From your results calculate the variance of these paramaters (recall that this is equal to the diagonal elements of the matrix $(\hat{X}^T\hat{X})+\lambda\hat{I})^{-1}$). Discuss the results of these variances as functions of $\lambda$. In particular, try to link your discussion with the discussion in Hastie *et al.* and their figure 3.11.

o Repeat the previous step but add now the Lasso method, see equation (3.53) of Hastie *et al.*. Discuss your results and compare with standard regression and the Ridge regression results. You can write your own code or use the functionality of _scikit-learn_.  We recommend the latter since we have not yet discussed how to solve the Lasso equations numerically.

o Finally, using _Scikit-Learn_ or your own code, compute also the mean square error, a risk metric corresponding to the expected value of the squared (quadratic) error defined as
!bt 
\[ MSE(\hat{y},\hat{\tilde{y}}) = \frac{1}{n}
\sum_{i=0}^{n-1}(y_i-\tilde{y}_i)^2, 
\] 
!et 
and the $R^2$ score function.
If $\tilde{\hat{y}}_i$ is the predicted value of the $i-th$ sample and $y_i$ is the corresponding true value, then the score $R^2$ is defined as
!bt
\[
R^2(\hat{y}, \tilde{\hat{y}}) = 1 - \frac{\sum_{i=0}^{n - 1} (y_i - \tilde{y}_i)^2}{\sum_{i=0}^{n - 1} (y_i - \bar{y})^2},
\]
!et
where we have defined the mean value  of $\hat{y}$ as
!bt
\[
\bar{y} =  \frac{1}{n} \sum_{i=0}^{n - 1} y_i.
\]
!et
Discuss these quantities as functions of the variable $\lambda$ in the Ridge and Lasso regression methods. 

===== Exercise 5  =====

The theory behind this exercise will be covered during the lectures of week 36. It requires reading chapter three of Hastie et al, in particular the derivations preceeding equation (3.49) as the well as the material in the Regression slides that deal with the singular value decomposition. 

Using the singular value decomposition, show that the variance of the direction vector 
$\hat{z}_i=\hat{X}\hat{v}_i=\hat{u}_1d_1$   is equal to (equation (3.49) of Hastie *et al.*)
!bt
\[
\mathrm{Var}(\hat{z}_i)=\frac{d_i^2}{N},
\]
!et
where $d_i$ are the singular values of the matrix $\hat{X}$. In Hastie *et al*, the matrix elements of $X$ are centered. The consequence is that the mean values of for example $\hat{u}_i$ are zero. 

Give an interpretation of these results, in particular in connection with the variance of the coefficients you obtained in the previous exercise.   


===== Solution to the last exercise =====

A possible way to show why $\left \langle \hat u_i \right \rangle = 0$
given that the columns of $\hat X$ is centered is by considering
$\left \langle \hat X \hat v_i \right \rangle$:

!bt 

\begin{align*} \left \langle \hat X \hat v_i \right \rangle &= \frac{1}{N}\sum_j ( \hat X \hat v_i )_j \\ &= \frac{1}{N}\sum_j \sum_k x_{jk}\hat v_i(k)\\ &= \frac{1}{N}\sum_k \hat v_i(k) \sum_j x_{jk} \\ &= \sum_k \hat v_i(k)\left( \frac{1}{N}\sum_j x_{jk} \right) \\ &= \sum_k \hat v_i(k) \left \langle \hat x_k \right \rangle
\end{align*}
!et

where $x_{jk}$ being the element of $\hat X$ at row $j$ and column
$k$, $( \hat X \hat v_i )_j $ the $j$-th element of the vector $\hat X
\hat v_i $, $\hat x_k$ being the $k$-th column vector of $\hat X$, and
$\hat v_i(k)$ the $k$-th element of the vector $\hat v_i$.



Since the columns of $\hat X$ are assumed to be centered, $\left
\langle \hat x_k \right \rangle = 0$ for all $k$. This gives that
$\left \langle \hat X \hat v_i \right \rangle = 0$.



But $\left \langle \hat X \hat v_i \right \rangle = \left \langle \hat
u_i d_i \right \rangle = d_i \left \langle \hat u_i \right \rangle $.

Since $ \left \langle \hat X \hat v_i \right \rangle = 0$, then $d_i
\left \langle \hat u_i \right \rangle = 0$ also. Assuming that $d_i
\neq 0$ (otherwise the variance in the exercise would just be zero),
gives that $\left \langle \hat u_i \right \rangle = 0$.



Regarding $\hat V$ and using the similar approach as above by
computing $\left \langle \hat X^T \hat u_i \right \rangle = d_i \left
\langle \hat v_i \right \rangle$, we have

!bt
\[
\begin{align*} \left \langle \hat X^T \hat u_i \right \rangle &= \frac{1}{N}\sum_j ( \hat X^T \hat u_i )_j \\ &= \frac{1}{N}\sum_j \sum_k x_{kj} \hat u_i(k)\\  &= \frac{1}{N}\sum_k \hat u_i(k) \sum_j x_{kj} \\ &= \frac{1}{N}\sum_k \hat u_i(k) \sum_j x_{kj} \\  &= \sum_k \hat u_i(k) \left( \frac{1}{N} \sum_j x_{kj} \right)\\ 
\end{align*}
!et

