<!--
Automatically generated HTML file from DocOnce source
(https://github.com/hplgit/doconce/)
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/hplgit/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="First and second days: Exercise set 1">

<title>First and second days: Exercise set 1</title>


<style type="text/css">
/* bloodish style */

body {
  font-family: Helvetica, Verdana, Arial, Sans-serif;
  color: #404040;
  background: #ffffff;
}
h1 { font-size: 1.8em;  color: #8A0808; }
h2 { font-size: 1.6em;  color: #8A0808; }
h3 { font-size: 1.4em;  color: #8A0808; }
h4 { color: #8A0808; }
a { color: #8A0808; text-decoration:none; }
tt { font-family: "Courier New", Courier; }
/* pre style removed because it will interfer with pygments */
p { text-indent: 0px; }
hr { border: 0; width: 80%; border-bottom: 1px solid #aaa}
p.caption { width: 80%; font-style: normal; text-align: left; }
hr.figure { border: 0; width: 80%; border-bottom: 1px solid #aaa}

div { text-align: justify; text-justify: inter-word; }
</style>


</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Day one and two  exercises', 2, None, '___sec0'),
              ('Exercise 1: Getting started', 2, None, '___sec1'),
              ('Exercise 2: Our first Python encounter', 2, None, '___sec2'),
              ('Exercise 3: making your own data and exploring scikit-learn',
               2,
               None,
               '___sec3'),
              ('Exercise 4: mean values and variances in linear regression',
               2,
               None,
               '___sec4'),
              ('Exercise 5: Playing with nuclear masses', 2, None, '___sec5')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "AMS"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



    
<!-- ------------------- main content ---------------------- -->



<center><h1>First and second days: Exercise set 1</h1></center>  <!-- document title -->

<p>
<!-- author(s): Data Analysis and Machine Learning for Nuclear Physics -->

<center>
<b>Data Analysis and Machine Learning for Nuclear Physics</b> 
</center>

<p>
<!-- institution(s) -->

<br>
<p>
<center><h4>Jan 18, 2021</h4></center> <!-- date -->
<br>

<h2 id="___sec0">Day one and two  exercises </h2>

<p>
<!-- --- begin exercise --- -->

<h2 id="___sec1">Exercise 1: Getting started </h2>

<p>
The first exercise here is of a mere technical art. We want you to have 

<ul>
<li> git as a version control software and to establish a user account on a provider like GitHub. Other providers like GitLab etc are equally fine.</li> 
<li> Install various Python packages</li>
</ul>

We will make extensive use of Python as programming language and its
myriad of available libraries.  You will find
IPython/Jupyter notebooks invaluable in your work.  You can run <b>R</b>
codes in the Jupyter/IPython notebooks, with the immediate benefit of
visualizing your data. You can also use compiled languages like C++,
Rust, Fortran etc if you prefer. The focus in these lectures will be
on Python.

<p>
If you have Python installed (we recommend Python3) and you feel
pretty familiar with installing different packages, we recommend that
you install the following Python packages via <b>pip</b> as 

<ol>
<li> pip install numpy scipy matplotlib ipython scikit-learn sympy pandas pillow</li> 
</ol>

For <b>Tensorflow</b>, we recommend following the instructions in the text of 
<a href="http://shop.oreilly.com/product/0636920052289.do" target="_blank">Aurelien Geron, Hands&#8209;On Machine Learning with Scikit&#8209;Learn and TensorFlow, O'Reilly</a>

<p>
We will come back to <b>tensorflow</b> later.

<p>
For Python3, replace <b>pip</b> with <b>pip3</b>.

<p>
For OSX users we recommend, after having installed Xcode, to
install <b>brew</b>. Brew allows for a seamless installation of additional
software via for example 

<ol>
<li> brew install python3</li>
</ol>

For Linux users, with its variety of distributions like for example the widely popular Ubuntu distribution,
you can use <b>pip</b> as well and simply install Python as 

<ol>
<li> sudo apt-get install python3  (or python for Python2.7)</li>
</ol>

If you don't want to perform these operations separately and venture
into the hassle of exploring how to set up dependencies and paths, we
recommend two widely used distrubutions which set up all relevant
dependencies for Python, namely 

<ul>
<li> <a href="https://docs.anaconda.com/" target="_blank">Anaconda</a>,</li> 
</ul>

which is an open source
distribution of the Python and R programming languages for large-scale
data processing, predictive analytics, and scientific computing, that
aims to simplify package management and deployment. Package versions
are managed by the package management system <b>conda</b>. 

<ul>
<li> <a href="https://www.enthought.com/product/canopy/" target="_blank">Enthought canopy</a></li> 
</ul>

is a Python
distribution for scientific and analytic computing distribution and
analysis environment, available for free and under a commercial
license.

<p>
We recommend using <b>Anaconda</b>.

<p>
<!-- --- end exercise --- -->

<p>
<!-- --- begin exercise --- -->

<h2 id="___sec2">Exercise 2: Our first Python encounter </h2>

<p>
This exercise has as its aim to write a small program which reads in data from a <b>csv</b> file on the equation of state for dense nuclear matter. The file is localized at <a href="https://github.com/mhjensen/MachineLearningMSU-FRIB2020/blob/master/doc/pub/Regression/ipynb/datafiles/EoS.csv" target="_blank"><tt>https://github.com/mhjensen/MachineLearningMSU-FRIB2020/blob/master/doc/pub/Regression/ipynb/datafiles/EoS.csv</tt></a>. Thereafter you will have to set up the design matrix \( \boldsymbol{X} \) for the  \( n \)
datapoints and a polynomial of degree \( 3 \). The steps are:

<ul>
<li> Write a Python code which reads the in the above mentioned file.</li>
<li> Use for example <b>pandas</b> to order your data and find out how many data points there are.</li>
<li> Set thereafter up the design matrix with dimensionality \( n\times p \) where \( p=4 \) and where you have defined a polynomial of degree \( p-1=3 \). Print the matrix and check that the numbers are correct.</li> 
</ul>

We recommend looking at the examples in the <a href="https://compphysics.github.io/MachineLearning/doc/pub/Regression/html/Regression-bs.html" target="_blank">regression slides</a>.

<p>
<!-- --- begin solution of exercise --- -->
<b>Solution.</b>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">os</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">pandas</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">pd</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span> train_test_split
<span style="color: #408080; font-style: italic"># Where to save the figures and data files</span>
PROJECT_ROOT_DIR <span style="color: #666666">=</span> <span style="color: #BA2121">&quot;Results&quot;</span>
FIGURE_ID <span style="color: #666666">=</span> <span style="color: #BA2121">&quot;Results/FigureFiles&quot;</span>
DATA_ID <span style="color: #666666">=</span> <span style="color: #BA2121">&quot;DataFiles/&quot;</span>

<span style="color: #008000; font-weight: bold">if</span> <span style="color: #AA22FF; font-weight: bold">not</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>exists(PROJECT_ROOT_DIR):
    os<span style="color: #666666">.</span>mkdir(PROJECT_ROOT_DIR)

<span style="color: #008000; font-weight: bold">if</span> <span style="color: #AA22FF; font-weight: bold">not</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>exists(FIGURE_ID):
    os<span style="color: #666666">.</span>makedirs(FIGURE_ID)

<span style="color: #008000; font-weight: bold">if</span> <span style="color: #AA22FF; font-weight: bold">not</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>exists(DATA_ID):
    os<span style="color: #666666">.</span>makedirs(DATA_ID)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">image_path</span>(fig_id):
    <span style="color: #008000; font-weight: bold">return</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>join(FIGURE_ID, fig_id)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">data_path</span>(dat_id):
    <span style="color: #008000; font-weight: bold">return</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>join(DATA_ID, dat_id)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">save_fig</span>(fig_id):
    plt<span style="color: #666666">.</span>savefig(image_path(fig_id) <span style="color: #666666">+</span> <span style="color: #BA2121">&quot;.png&quot;</span>, <span style="color: #008000">format</span><span style="color: #666666">=</span><span style="color: #BA2121">&#39;png&#39;</span>)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">R2</span>(y_data, y_model):
    <span style="color: #008000; font-weight: bold">return</span> <span style="color: #666666">1</span> <span style="color: #666666">-</span> np<span style="color: #666666">.</span>sum((y_data <span style="color: #666666">-</span> y_model) <span style="color: #666666">**</span> <span style="color: #666666">2</span>) <span style="color: #666666">/</span> np<span style="color: #666666">.</span>sum((y_data <span style="color: #666666">-</span> np<span style="color: #666666">.</span>mean(y_data)) <span style="color: #666666">**</span> <span style="color: #666666">2</span>)
<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">MSE</span>(y_data,y_model):
    n <span style="color: #666666">=</span> np<span style="color: #666666">.</span>size(y_model)
    <span style="color: #008000; font-weight: bold">return</span> np<span style="color: #666666">.</span>sum((y_data<span style="color: #666666">-</span>y_model)<span style="color: #666666">**2</span>)<span style="color: #666666">/</span>n

infile <span style="color: #666666">=</span> <span style="color: #008000">open</span>(data_path(<span style="color: #BA2121">&quot;EoS.csv&quot;</span>),<span style="color: #BA2121">&#39;r&#39;</span>)

<span style="color: #408080; font-style: italic"># Read the EoS data as  csv file and organized into two arrays with density and energies</span>
EoS <span style="color: #666666">=</span> pd<span style="color: #666666">.</span>read_csv(infile, names<span style="color: #666666">=</span>(<span style="color: #BA2121">&#39;Density&#39;</span>, <span style="color: #BA2121">&#39;Energy&#39;</span>))
EoS[<span style="color: #BA2121">&#39;Energy&#39;</span>] <span style="color: #666666">=</span> pd<span style="color: #666666">.</span>to_numeric(EoS[<span style="color: #BA2121">&#39;Energy&#39;</span>], errors<span style="color: #666666">=</span><span style="color: #BA2121">&#39;coerce&#39;</span>)
EoS <span style="color: #666666">=</span> EoS<span style="color: #666666">.</span>dropna()
Energies <span style="color: #666666">=</span> EoS[<span style="color: #BA2121">&#39;Energy&#39;</span>]
Density <span style="color: #666666">=</span> EoS[<span style="color: #BA2121">&#39;Density&#39;</span>]
<span style="color: #408080; font-style: italic">#  The design matrix now as function of various polytrops</span>
X <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros((<span style="color: #008000">len</span>(Density),<span style="color: #666666">5</span>))
X[:,<span style="color: #666666">0</span>] <span style="color: #666666">=</span> <span style="color: #666666">1</span>
X[:,<span style="color: #666666">1</span>] <span style="color: #666666">=</span> Density<span style="color: #666666">**</span>(<span style="color: #666666">2.0/3.0</span>)
X[:,<span style="color: #666666">2</span>] <span style="color: #666666">=</span> Density
X[:,<span style="color: #666666">3</span>] <span style="color: #666666">=</span> Density<span style="color: #666666">**</span>(<span style="color: #666666">4.0/3.0</span>)
X[:,<span style="color: #666666">4</span>] <span style="color: #666666">=</span> Density<span style="color: #666666">**</span>(<span style="color: #666666">5.0/3.0</span>)
<span style="color: #408080; font-style: italic"># We split the data in test and training data</span>
X_train, X_test, y_train, y_test <span style="color: #666666">=</span> train_test_split(X, Energies, test_size<span style="color: #666666">=0.2</span>)
<span style="color: #408080; font-style: italic"># matrix inversion to find beta</span>
beta <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linalg<span style="color: #666666">.</span>inv(X_train<span style="color: #666666">.</span>T <span style="color: #666666">@</span> X_train) <span style="color: #666666">@</span> X_train<span style="color: #666666">.</span>T <span style="color: #666666">@</span> y_train
<span style="color: #408080; font-style: italic"># and then make the prediction</span>
ytilde <span style="color: #666666">=</span> X_train <span style="color: #666666">@</span> beta
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Training R2&quot;</span>)
<span style="color: #008000">print</span>(R2(y_train,ytilde))
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Training MSE&quot;</span>)
<span style="color: #008000">print</span>(MSE(y_train,ytilde))
ypredict <span style="color: #666666">=</span> X_test <span style="color: #666666">@</span> beta
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Test R2&quot;</span>)
<span style="color: #008000">print</span>(R2(y_test,ypredict))
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Test MSE&quot;</span>)
<span style="color: #008000">print</span>(MSE(y_test,ypredict))
</pre></div>
<p>
<!-- --- end solution of exercise --- -->

<p>
<!-- --- end exercise --- -->

<p>
<!-- --- begin exercise --- -->

<h2 id="___sec3">Exercise 3: making your own data and exploring scikit-learn </h2>

<p>
We will generate our own dataset for a function \( y(x) \) where \( x \in [0,1] \) and defined by random numbers computed with the uniform distribution. The function \( y \) is a quadratic polynomial in \( x \) with added stochastic noise according to the normal distribution \( N(0,1) \).
The following simple Python instructions define our \( x \) and \( y \) values (with 100 data points).
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span>x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>rand(<span style="color: #666666">100</span>,<span style="color: #666666">1</span>)
y <span style="color: #666666">=</span> <span style="color: #666666">2.0+5*</span>x<span style="color: #666666">*</span>x<span style="color: #666666">+0.1*</span>np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>randn(<span style="color: #666666">100</span>,<span style="color: #666666">1</span>)
</pre></div>
<ol>
<li> Write your own code (following the examples under the <a href="https://compphysics.github.io/MachineLearningECT/doc/pub/Day1/html/Day1-bs.html" target="_blank">regression slides</a>) for computing the parametrization of the data set fitting a second-order polynomial.</li> 
<li> Use thereafter <b>scikit-learn</b> (see again the examples in the regression slides) and compare with your own code.</li>   
<li> Using scikit-learn, compute also the mean square error, a risk metric corresponding to the expected value of the squared (quadratic) error defined as</li>
</ol>

$$ MSE(\boldsymbol{y},\boldsymbol{\tilde{y}}) = \frac{1}{n}
\sum_{i=0}^{n-1}(y_i-\tilde{y}_i)^2, 
$$

and the \( R^2 \) score function.
If \( \tilde{y}_i \) is the predicted value of the \( i-th \) sample and \( y_i \) is the corresponding true value, then the score \( R^2 \) is defined as
$$
R^2(\boldsymbol{y}, \tilde{\boldsymbol{y}}) = 1 - \frac{\sum_{i=0}^{n - 1} (y_i - \tilde{y}_i)^2}{\sum_{i=0}^{n - 1} (y_i - \bar{y})^2},
$$

where we have defined the mean value  of \( \hat{y} \) as
$$
\bar{y} =  \frac{1}{n} \sum_{i=0}^{n - 1} y_i.
$$

You can use the functionality included in scikit-learn. If you feel for it, you can use your own program and define functions which compute the above two functions. 
Discuss the meaning of these results. Try also to vary the coefficient in front of the added stochastic noise term and discuss the quality of the fits.

<p>
<!-- --- begin solution of exercise --- -->
<b>Solution.</b>
The code here is an example of where we define our own design matrix and fit parameters \( \beta \).
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">os</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">pandas</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">pd</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span> train_test_split

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">save_fig</span>(fig_id):
    plt<span style="color: #666666">.</span>savefig(image_path(fig_id) <span style="color: #666666">+</span> <span style="color: #BA2121">&quot;.png&quot;</span>, <span style="color: #008000">format</span><span style="color: #666666">=</span><span style="color: #BA2121">&#39;png&#39;</span>)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">R2</span>(y_data, y_model):
    <span style="color: #008000; font-weight: bold">return</span> <span style="color: #666666">1</span> <span style="color: #666666">-</span> np<span style="color: #666666">.</span>sum((y_data <span style="color: #666666">-</span> y_model) <span style="color: #666666">**</span> <span style="color: #666666">2</span>) <span style="color: #666666">/</span> np<span style="color: #666666">.</span>sum((y_data <span style="color: #666666">-</span> np<span style="color: #666666">.</span>mean(y_data)) <span style="color: #666666">**</span> <span style="color: #666666">2</span>)
<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">MSE</span>(y_data,y_model):
    n <span style="color: #666666">=</span> np<span style="color: #666666">.</span>size(y_model)
    <span style="color: #008000; font-weight: bold">return</span> np<span style="color: #666666">.</span>sum((y_data<span style="color: #666666">-</span>y_model)<span style="color: #666666">**2</span>)<span style="color: #666666">/</span>n

x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>rand(<span style="color: #666666">100</span>)
y <span style="color: #666666">=</span> <span style="color: #666666">2.0+5*</span>x<span style="color: #666666">*</span>x<span style="color: #666666">+0.1*</span>np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>randn(<span style="color: #666666">100</span>)


<span style="color: #408080; font-style: italic">#  The design matrix now as function of a given polynomial</span>
X <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros((<span style="color: #008000">len</span>(x),<span style="color: #666666">3</span>))
X[:,<span style="color: #666666">0</span>] <span style="color: #666666">=</span> <span style="color: #666666">1.0</span>
X[:,<span style="color: #666666">1</span>] <span style="color: #666666">=</span> x
X[:,<span style="color: #666666">2</span>] <span style="color: #666666">=</span> x<span style="color: #666666">**2</span>
<span style="color: #408080; font-style: italic"># We split the data in test and training data</span>
X_train, X_test, y_train, y_test <span style="color: #666666">=</span> train_test_split(X, y, test_size<span style="color: #666666">=0.2</span>)
<span style="color: #408080; font-style: italic"># matrix inversion to find beta</span>
beta <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linalg<span style="color: #666666">.</span>inv(X_train<span style="color: #666666">.</span>T <span style="color: #666666">@</span> X_train) <span style="color: #666666">@</span> X_train<span style="color: #666666">.</span>T <span style="color: #666666">@</span> y_train
<span style="color: #008000">print</span>(beta)
<span style="color: #408080; font-style: italic"># and then make the prediction</span>
ytilde <span style="color: #666666">=</span> X_train <span style="color: #666666">@</span> beta
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Training R2&quot;</span>)
<span style="color: #008000">print</span>(R2(y_train,ytilde))
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Training MSE&quot;</span>)
<span style="color: #008000">print</span>(MSE(y_train,ytilde))
ypredict <span style="color: #666666">=</span> X_test <span style="color: #666666">@</span> beta
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Test R2&quot;</span>)
<span style="color: #008000">print</span>(R2(y_test,ypredict))
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Test MSE&quot;</span>)
<span style="color: #008000">print</span>(MSE(y_test,ypredict))
</pre></div>
<p>
<!-- --- end solution of exercise --- -->

<p>
<!-- --- end exercise --- -->

<p>
<!-- --- begin exercise --- -->

<h2 id="___sec4">Exercise 4: mean values and variances in linear regression </h2>

<p>
This exercise deals with various mean values ad variances in  linear regression method (here it may be useful to look up chapter 3, equation (3.8) of <a href="https://www.springer.com/gp/book/9780387848570" target="_blank">Trevor Hastie, Robert Tibshirani, Jerome H. Friedman, The Elements of Statistical Learning, Springer</a>).

<p>
The assumption we have made is 
that there exists a function \( f(\boldsymbol{x}) \) and  a normal distributed error \( \boldsymbol{\varepsilon}\sim \mathcal{N}(0, \sigma^2) \)
which describes our data
$$
\boldsymbol{y} = f(\boldsymbol{x})+\boldsymbol{\varepsilon}
$$

<p>
We then approximate this function with our model from the solution of the linear regression equations (ordinary least squares OLS), that is our
function \( f \) is approximated by \( \boldsymbol{\tilde{y}} \) where we minimized  \( (\boldsymbol{y}-\boldsymbol{\tilde{y}})^2 \), with
$$
f(\boldsymbol{x})\approx \boldsymbol{\tilde{y}} = \boldsymbol{X}\boldsymbol{\beta}.
$$

The matrix \( \boldsymbol{X} \) is the so-called design matrix.

<p>
<b>a)</b>
Show that  the expected value of \( \boldsymbol{y} \) for a given element \( i \) 
$$
\begin{align*} 
\mathbb{E}(y_i) & =\mathbf{X}_{i, \ast} \, \beta, 
\end{align*} 
$$

and that
its variance is 
$$
\begin{align*} \mbox{Var}(y_i) & = \sigma^2.  
\end{align*}
$$

Hence, \( y_i \sim \mathcal{N}( \mathbf{X}_{i, \ast} \, \boldsymbol{\beta}, \sigma^2) \), that is \( \boldsymbol{y} \) follows a normal distribution with 
mean value \( \boldsymbol{X}\boldsymbol{\beta} \) and variance \( \sigma^2 \).

<p>
<!-- --- begin solution of exercise --- -->
<b>Solution.</b>
We can calculate the expected value of \( \boldsymbol{y} \) for a given element \( i \) 
$$
\begin{align*} 
\mathbb{E}(y_i) & =
\mathbb{E}(\mathbf{X}_{i, \ast} \, \boldsymbol{\beta}) + \mathbb{E}(\varepsilon_i)
\, \, \, = \, \, \, \mathbf{X}_{i, \ast} \, \beta, 
\end{align*} 
$$

while
its variance is 
$$
\begin{align*} \mbox{Var}(y_i) & = \mathbb{E} \{ [y_i
- \mathbb{E}(y_i)]^2 \} \, \, \, = \, \, \, \mathbb{E} ( y_i^2 ) -
[\mathbb{E}(y_i)]^2  \\  & = \mathbb{E} [ ( \mathbf{X}_{i, \ast} \,
\beta + \varepsilon_i )^2] - ( \mathbf{X}_{i, \ast} \, \boldsymbol{\beta})^2 \\ &
= \mathbb{E} [ ( \mathbf{X}_{i, \ast} \, \boldsymbol{\beta})^2 + 2 \varepsilon_i
\mathbf{X}_{i, \ast} \, \boldsymbol{\beta} + \varepsilon_i^2 ] - ( \mathbf{X}_{i,
\ast} \, \beta)^2 \\  & = ( \mathbf{X}_{i, \ast} \, \boldsymbol{\beta})^2 + 2
\mathbb{E}(\varepsilon_i) \mathbf{X}_{i, \ast} \, \boldsymbol{\beta} +
\mathbb{E}(\varepsilon_i^2 ) - ( \mathbf{X}_{i, \ast} \, \boldsymbol{\beta})^2 
\\ & = \mathbb{E}(\varepsilon_i^2 ) \, \, \, = \, \, \,
\mbox{Var}(\varepsilon_i) \, \, \, = \, \, \, \sigma^2.  
\end{align*}
$$

Hence, \( y_i \sim \mathcal{N}( \mathbf{X}_{i, \ast} \, \boldsymbol{\beta}, \sigma^2) \), that is \( \boldsymbol{y} \) follows a normal distribution with 
mean value \( \boldsymbol{X}\boldsymbol{\beta} \) and variance \( \sigma^2 \) (not be confused with the singular values of the SVD).

<p>
<!-- --- end solution of exercise --- -->

<p>
<b>b)</b>
With the OLS expressions for the optimal parameters \( \boldsymbol{\beta}^{\mathrm{opt}} \) show that
$$
\mathbb{E}(\boldsymbol{\beta}^{\mathrm{opt}}) = \boldsymbol{\beta}.
$$

<p>
<!-- --- begin solution of exercise --- -->
<b>Solution.</b>
$$
\mathbb{E}(\boldsymbol{\beta}^{\mathrm{opt}}) = \mathbb{E}[ (\mathbf{X}^{\top} \mathbf{X})^{-1}\mathbf{X}^{T} \mathbf{Y}]=(\mathbf{X}^{T} \mathbf{X})^{-1}\mathbf{X}^{T} \mathbb{E}[ \mathbf{Y}]=(\mathbf{X}^{T} \mathbf{X})^{-1} \mathbf{X}^{T}\mathbf{X}\boldsymbol{\beta}=\boldsymbol{\beta}.
$$

This means that the estimator of the regression parameters is unbiased.

<p>
<!-- --- end solution of exercise --- -->

<p>
<b>c)</b>
Show finally that the variance of \( \boldsymbol{\beta} \) is
$$
\begin{eqnarray*}
\mbox{Var}(\boldsymbol{\beta}^{\mathrm{opt}}) & = & \sigma^2 \, (\mathbf{X}^{T} \mathbf{X})^{-1}.
\end{eqnarray*}
$$

<p>
<!-- --- begin solution of exercise --- -->
<b>Solution.</b>
The variance of \( \boldsymbol{\beta} \) is
$$
\begin{eqnarray*}
\mbox{Var}(\boldsymbol{\beta}^{\mathrm{opt}}) & = & \mathbb{E} \{ [\boldsymbol{\beta} - \mathbb{E}(\boldsymbol{\beta})] [\boldsymbol{\beta} - \mathbb{E}(\boldsymbol{\beta})]^{T} \}
\\
& = & \mathbb{E} \{ [(\mathbf{X}^{T} \mathbf{X})^{-1} \, \mathbf{X}^{T} \mathbf{Y} - \boldsymbol{\beta}] \, [(\mathbf{X}^{T} \mathbf{X})^{-1} \, \mathbf{X}^{T} \mathbf{Y} - \boldsymbol{\beta}]^{T} \}
\\
% & = & \mathbb{E} \{ [(\mathbf{X}^{T} \mathbf{X})^{-1} \, \mathbf{X}^{T} \mathbf{Y}] \, [(\mathbf{X}^{T} \mathbf{X})^{-1} \, \mathbf{X}^{T} \mathbf{Y}]^{T} \} - \boldsymbol{\beta} \, \boldsymbol{\beta}^{T}
% \\
% & = & \mathbb{E} \{ (\mathbf{X}^{T} \mathbf{X})^{-1} \, \mathbf{X}^{T} \mathbf{Y} \, \mathbf{Y}^{T} \, \mathbf{X} \, (\mathbf{X}^{T} \mathbf{X})^{-1}  \} - \boldsymbol{\beta} \, \boldsymbol{\beta}^{T}
% \\
& = & (\mathbf{X}^{T} \mathbf{X})^{-1} \, \mathbf{X}^{T} \, \mathbb{E} \{ \mathbf{Y} \, \mathbf{Y}^{T} \} \, \mathbf{X} \, (\mathbf{X}^{T} \mathbf{X})^{-1} - \boldsymbol{\beta} \, \boldsymbol{\beta}^{T}
\\
& = & (\mathbf{X}^{T} \mathbf{X})^{-1} \, \mathbf{X}^{T} \, \{ \mathbf{X} \, \boldsymbol{\beta} \, \boldsymbol{\beta}^{T} \,  \mathbf{X}^{T} + \sigma^2 \} \, \mathbf{X} \, (\mathbf{X}^{T} \mathbf{X})^{-1} - \boldsymbol{\beta} \, \boldsymbol{\beta}^{T}
% \\
% & = & (\mathbf{X}^T \mathbf{X})^{-1} \, \mathbf{X}^T \, \mathbf{X} \, \boldsymbol{\beta} \, \boldsymbol{\beta}^T \,  \mathbf{X}^T \, \mathbf{X} \, (\mathbf{X}^T % \mathbf{X})^{-1}
% \\
% & & + \, \, \sigma^2 \, (\mathbf{X}^T \mathbf{X})^{-1} \, \mathbf{X}^T  \, \mathbf{X} \, (\mathbf{X}^T \mathbf{X})^{-1} - \boldsymbol{\beta} \boldsymbol{\beta}^T
\\
& = & \boldsymbol{\beta} \, \boldsymbol{\beta}^{T}  + \sigma^2 \, (\mathbf{X}^{T} \mathbf{X})^{-1} - \boldsymbol{\beta} \, \boldsymbol{\beta}^{T}
\, \, \, = \, \, \, \sigma^2 \, (\mathbf{X}^{T} \mathbf{X})^{-1},
\end{eqnarray*}
$$

<p>
where we have used  that \( \mathbb{E} (\mathbf{Y} \mathbf{Y}^{T}) =
\mathbf{X} \, \boldsymbol{\beta} \, \boldsymbol{\beta}^{T} \, \mathbf{X}^{T} +
\sigma^2 \, \mathbf{I}_{nn} \). From \( \mbox{Var}(\boldsymbol{\beta}) = \sigma^2
\, (\mathbf{X}^{T} \mathbf{X})^{-1} \), one obtains an estimate of the
variance of the estimate of the \( j \)-th regression coefficient:
\( \boldsymbol{\sigma}^2 (\hat{\beta}_j ) = \boldsymbol{\sigma}^2 \sqrt{
[(\mathbf{X}^{T} \mathbf{X})^{-1}]_{jj} } \). This may be used to
construct a confidence interval for the estimates.

<p>
In a similar way, we can obtain analytical expressions for say the
expectation values of the parameters \( \boldsymbol{\beta} \) and their variance
when we employ Ridge regression, allowing us again to define a confidence interval.

<p>
<!-- --- end solution of exercise --- -->

<p>
<!-- --- end exercise --- -->

<p>
<!-- --- begin exercise --- -->

<h2 id="___sec5">Exercise 5: Playing with nuclear masses </h2>

<p>
Finally, try now to write your own code (you can use the example the nuclear masses in the lecture slides on Regression and Getting started from Day1, that reads in the nuclear masses and
compute the proton separation energies, the two-neutron and two-proton separation energies and finally the shell gaps for selected nuclei.

<p>
Finally, try to compute the \( Q \)-values for \( \beta- \) decay for selected nuclei.

<p>
<!-- --- begin solution of exercise --- -->
<b>Solution.</b>
Let us study the \( Q \) values associated with the removal of one or two nucleons from
a nucleus. These are conventionally defined in terms of the one-nucleon and two-nucleon
separation energies. With the functionality in <b>pandas</b>, two to three lines of code will allow us to plot the separation energies.
The neutron separation energy is defined as 

$$
S_n= -Q_n= BE(N,Z)-BE(N-1,Z),
$$

and the proton separation energy reads
$$
S_p= -Q_p= BE(N,Z)-BE(N,Z-1).
$$

The two-neutron separation energy is defined as
$$
S_{2n}= -Q_{2n}= BE(N,Z)-BE(N-2,Z),
$$

and  the two-proton separation energy is given by
$$
S_{2p}= -Q_{2p}= BE(N,Z)-BE(N,Z-2).
$$

<p>
Using say the neutron separation energies (alternatively the proton separation energies)
$$
S_n= -Q_n= BE(N,Z)-BE(N-1,Z),
$$

we can define the so-called energy gap for neutrons (or protons) as 
$$
\Delta S_n= BE(N,Z)-BE(N-1,Z)-\left(BE(N+1,Z)-BE(N,Z)\right),
$$

or 
$$
\Delta S_n= 2BE(N,Z)-BE(N-1,Z)-BE(N+1,Z).
$$

This quantity can in turn be used to determine which nuclei could be interpreted as  magic or not. 
For protons we would have 
$$
\Delta S_p= 2BE(N,Z)-BE(N,Z-1)-BE(N,Z+1).
$$

<p>
To calculate say the neutron separation we need to multiply our masses with the nucleon number \( A \).
The example here is for the neutron separation energies for the oxygen isotopes.
Note the simple function we use to compute the neutron separation energies
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #408080; font-style: italic"># Her we pick the oyxgen isotopes</span>
Nucleus <span style="color: #666666">=</span> df<span style="color: #666666">.</span>loc[<span style="color: #008000; font-weight: bold">lambda</span> df: df<span style="color: #666666">.</span>Z<span style="color: #666666">==8</span>, :]
<span style="color: #408080; font-style: italic"># drop cases with no number</span>
Nucleus <span style="color: #666666">=</span> Nucleus<span style="color: #666666">.</span>dropna()
<span style="color: #408080; font-style: italic"># Here we do the magic and obtain the neutron separation energies, one line of code!!</span>
Nucleus[<span style="color: #BA2121">&#39;NeutronSeparationEnergies&#39;</span>] <span style="color: #666666">=</span> Nucleus[<span style="color: #BA2121">&#39;Energies&#39;</span>]<span style="color: #666666">.</span>diff(<span style="color: #666666">+1</span>)
</pre></div>
<p>
If we want another isotope we need simply to change the \( Z \) value. For isotones, we fix simply the neutron number. Furthermore, if we wish to compute say the two-neutron separation energies of the oyxgen isotopes we need simply to write 
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #408080; font-style: italic"># Her we pick the oyxgen isotopes</span>
Nucleus <span style="color: #666666">=</span> df<span style="color: #666666">.</span>loc[<span style="color: #008000; font-weight: bold">lambda</span> df: df<span style="color: #666666">.</span>Z<span style="color: #666666">==8</span>, :]
<span style="color: #408080; font-style: italic"># drop cases with no number</span>
Nucleus <span style="color: #666666">=</span> Nucleus<span style="color: #666666">.</span>dropna()
<span style="color: #408080; font-style: italic"># Here we do the magic and obtain the neutron separation energies, one line of code!!</span>
Nucleus[<span style="color: #BA2121">&#39;NeutronSeparationEnergies&#39;</span>] <span style="color: #666666">=</span> Nucleus[<span style="color: #BA2121">&#39;Energies&#39;</span>]<span style="color: #666666">.</span>diff(<span style="color: #666666">+2</span>)
</pre></div>
<p>
Note the \( +2 \) in the function <b>diff(+2)</b>!  Easy, isn't it? It is easy to change to two-proton separation energies. The full example here is for the neutron separation energies.

<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #408080; font-style: italic"># Common imports</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">pandas</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">pd</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">os</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">pylab</span> <span style="color: #008000; font-weight: bold">import</span> plt, mpl
plt<span style="color: #666666">.</span>style<span style="color: #666666">.</span>use(<span style="color: #BA2121">&#39;seaborn&#39;</span>)
mpl<span style="color: #666666">.</span>rcParams[<span style="color: #BA2121">&#39;font.family&#39;</span>] <span style="color: #666666">=</span> <span style="color: #BA2121">&#39;serif&#39;</span>


<span style="color: #408080; font-style: italic"># Where to save the figures and data files</span>
PROJECT_ROOT_DIR <span style="color: #666666">=</span> <span style="color: #BA2121">&quot;Results&quot;</span>
FIGURE_ID <span style="color: #666666">=</span> <span style="color: #BA2121">&quot;Results/FigureFiles&quot;</span>
DATA_ID <span style="color: #666666">=</span> <span style="color: #BA2121">&quot;DataFiles/&quot;</span>

<span style="color: #008000; font-weight: bold">if</span> <span style="color: #AA22FF; font-weight: bold">not</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>exists(PROJECT_ROOT_DIR):
    os<span style="color: #666666">.</span>mkdir(PROJECT_ROOT_DIR)

<span style="color: #008000; font-weight: bold">if</span> <span style="color: #AA22FF; font-weight: bold">not</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>exists(FIGURE_ID):
    os<span style="color: #666666">.</span>makedirs(FIGURE_ID)

<span style="color: #008000; font-weight: bold">if</span> <span style="color: #AA22FF; font-weight: bold">not</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>exists(DATA_ID):
    os<span style="color: #666666">.</span>makedirs(DATA_ID)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">image_path</span>(fig_id):
    <span style="color: #008000; font-weight: bold">return</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>join(FIGURE_ID, fig_id)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">data_path</span>(dat_id):
    <span style="color: #008000; font-weight: bold">return</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>join(DATA_ID, dat_id)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">save_fig</span>(fig_id):
    plt<span style="color: #666666">.</span>savefig(image_path(fig_id) <span style="color: #666666">+</span> <span style="color: #BA2121">&quot;.png&quot;</span>, <span style="color: #008000">format</span><span style="color: #666666">=</span><span style="color: #BA2121">&#39;png&#39;</span>)

infile <span style="color: #666666">=</span> <span style="color: #008000">open</span>(data_path(<span style="color: #BA2121">&quot;MassEval2016.dat&quot;</span>),<span style="color: #BA2121">&#39;r&#39;</span>)


<span style="color: #408080; font-style: italic"># Read the experimental data with Pandas</span>
Masses <span style="color: #666666">=</span> pd<span style="color: #666666">.</span>read_fwf(infile, usecols<span style="color: #666666">=</span>(<span style="color: #666666">2</span>,<span style="color: #666666">3</span>,<span style="color: #666666">4</span>,<span style="color: #666666">6</span>,<span style="color: #666666">11</span>),
              names<span style="color: #666666">=</span>(<span style="color: #BA2121">&#39;N&#39;</span>, <span style="color: #BA2121">&#39;Z&#39;</span>, <span style="color: #BA2121">&#39;A&#39;</span>, <span style="color: #BA2121">&#39;Element&#39;</span>, <span style="color: #BA2121">&#39;Ebinding&#39;</span>),
              widths<span style="color: #666666">=</span>(<span style="color: #666666">1</span>,<span style="color: #666666">3</span>,<span style="color: #666666">5</span>,<span style="color: #666666">5</span>,<span style="color: #666666">5</span>,<span style="color: #666666">1</span>,<span style="color: #666666">3</span>,<span style="color: #666666">4</span>,<span style="color: #666666">1</span>,<span style="color: #666666">13</span>,<span style="color: #666666">11</span>,<span style="color: #666666">11</span>,<span style="color: #666666">9</span>,<span style="color: #666666">1</span>,<span style="color: #666666">2</span>,<span style="color: #666666">11</span>,<span style="color: #666666">9</span>,<span style="color: #666666">1</span>,<span style="color: #666666">3</span>,<span style="color: #666666">1</span>,<span style="color: #666666">12</span>,<span style="color: #666666">11</span>,<span style="color: #666666">1</span>),
              header<span style="color: #666666">=39</span>,
              index_col<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>)

<span style="color: #408080; font-style: italic"># Extrapolated values are indicated by &#39;#&#39; in place of the decimal place, so</span>
<span style="color: #408080; font-style: italic"># the Ebinding column won&#39;t be numeric. Coerce to float and drop these entries.</span>
Masses[<span style="color: #BA2121">&#39;Ebinding&#39;</span>] <span style="color: #666666">=</span> pd<span style="color: #666666">.</span>to_numeric(Masses[<span style="color: #BA2121">&#39;Ebinding&#39;</span>], errors<span style="color: #666666">=</span><span style="color: #BA2121">&#39;coerce&#39;</span>)
Masses <span style="color: #666666">=</span> Masses<span style="color: #666666">.</span>dropna()
<span style="color: #408080; font-style: italic"># Convert from keV to MeV.</span>
Masses[<span style="color: #BA2121">&#39;Ebinding&#39;</span>] <span style="color: #666666">/=</span> <span style="color: #666666">1000</span>
A <span style="color: #666666">=</span> Masses[<span style="color: #BA2121">&#39;A&#39;</span>]
Z <span style="color: #666666">=</span> Masses[<span style="color: #BA2121">&#39;Z&#39;</span>]
N <span style="color: #666666">=</span> Masses[<span style="color: #BA2121">&#39;N&#39;</span>]
Element <span style="color: #666666">=</span> Masses[<span style="color: #BA2121">&#39;Element&#39;</span>]
Energies <span style="color: #666666">=</span> Masses[<span style="color: #BA2121">&#39;Ebinding&#39;</span>]<span style="color: #666666">*</span>A

df <span style="color: #666666">=</span> pd<span style="color: #666666">.</span>DataFrame({<span style="color: #BA2121">&#39;A&#39;</span>:A,<span style="color: #BA2121">&#39;Z&#39;</span>:Z, <span style="color: #BA2121">&#39;N&#39;</span>:N,<span style="color: #BA2121">&#39;Element&#39;</span>:Element,<span style="color: #BA2121">&#39;Energies&#39;</span>:Energies})
<span style="color: #408080; font-style: italic"># Her we pick the oyxgen isotopes</span>
Nucleus <span style="color: #666666">=</span> df<span style="color: #666666">.</span>loc[<span style="color: #008000; font-weight: bold">lambda</span> df: df<span style="color: #666666">.</span>Z<span style="color: #666666">==8</span>, :]
<span style="color: #408080; font-style: italic"># drop cases with no number</span>
Nucleus <span style="color: #666666">=</span> Nucleus<span style="color: #666666">.</span>dropna()
<span style="color: #408080; font-style: italic"># Here we do the magic and obtain the neutron separation energies, one line of code!!</span>
Nucleus[<span style="color: #BA2121">&#39;NeutronSeparationEnergies&#39;</span>] <span style="color: #666666">=</span> Nucleus[<span style="color: #BA2121">&#39;Energies&#39;</span>]<span style="color: #666666">.</span>diff(<span style="color: #666666">+1</span>)
<span style="color: #008000">print</span>(Nucleus)
MakePlot([Nucleus<span style="color: #666666">.</span>A], [Nucleus<span style="color: #666666">.</span>NeutronSeparationEnergies], [<span style="color: #BA2121">&#39;b&#39;</span>], [<span style="color: #BA2121">&#39;Neutron Separation Energy&#39;</span>], [<span style="color: #BA2121">&#39;$A$&#39;</span>,<span style="color: #BA2121">&#39;$S_n$&#39;</span>])
save_fig(<span style="color: #BA2121">&#39;Nucleus&#39;</span>)
plt<span style="color: #666666">.</span>show()
</pre></div>
<p>
<!-- --- end solution of exercise --- -->

<p>
<!-- --- end exercise --- -->

<!-- ------------------- end of main content --------------- -->


</body>
</html>
    

