<!--
HTML file automatically generated from DocOnce source
(https://github.com/doconce/doconce/)
doconce format html hw3.do.txt --html_style=bootstrap --pygments_html_style=default --html_admon=bootstrap_panel --html_output=hw3-bs
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/doconce/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Fourth and fifth days: Homework set 3">
<title>Fourth and fifth days: Homework set 3</title>
<!-- Bootstrap style: bootstrap -->
<!-- doconce format html hw3.do.txt --html_style=bootstrap --pygments_html_style=default --html_admon=bootstrap_panel --html_output=hw3-bs -->
<link href="https://netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css" rel="stylesheet">
<!-- not necessary
<link href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
-->
<style type="text/css">
/* Add scrollbar to dropdown menus in bootstrap navigation bar */
.dropdown-menu {
   height: auto;
   max-height: 400px;
   overflow-x: hidden;
}
/* Adds an invisible element before each target to offset for the navigation
   bar */
.anchor::before {
  content:"";
  display:block;
  height:50px;      /* fixed header height for style bootstrap */
  margin:-50px 0 0; /* negative fixed header height */
}
</style>
</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Day four and five exercises',
               2,
               None,
               'day-four-and-five-exercises'),
              ('Exercise 1, Bias-Variance tradeoff and Bootstrap',
               3,
               None,
               'exercise-1-bias-variance-tradeoff-and-bootstrap'),
              ('Part (1a) Proving the bias-variance tradeoff',
               3,
               None,
               'part-1a-proving-the-bias-variance-tradeoff'),
              ('Part (1b) Adding Bootstrap and Bias-Variance Tradeoff',
               3,
               None,
               'part-1b-adding-bootstrap-and-bias-variance-tradeoff'),
              ('Exercise 2, Linear Regression for  a two-dimensional function',
               3,
               None,
               'exercise-2-linear-regression-for-a-two-dimensional-function'),
              ('(2a) Ordinary Least Square on the Franke function  with '
               'resampling',
               3,
               None,
               '2a-ordinary-least-square-on-the-franke-function-with-resampling'),
              ('Part (2b) Resampling techniques, adding more complexity',
               3,
               None,
               'part-2b-resampling-techniques-adding-more-complexity'),
              ('Part (2c): Bias-variance tradeoff',
               3,
               None,
               'part-2c-bias-variance-tradeoff'),
              ('Part (2d): Ridge Regression on the Franke function  with '
               'resampling',
               3,
               None,
               'part-2d-ridge-regression-on-the-franke-function-with-resampling'),
              ('Part (2e): Lasso Regression on the Franke function  with '
               'resampling',
               3,
               None,
               'part-2e-lasso-regression-on-the-franke-function-with-resampling')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "none"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<!-- Bootstrap navigation bar -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-responsive-collapse">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="w3-bs.html">Fourth and fifth days: Homework set 3</a>
  </div>
  <div class="navbar-collapse collapse navbar-responsive-collapse">
    <ul class="nav navbar-nav navbar-right">
      <li class="dropdown">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Contents <b class="caret"></b></a>
        <ul class="dropdown-menu">
     <!-- navigation toc: --> <li><a href="#day-four-and-five-exercises" style="font-size: 80%;"><b>Day four and five exercises</b></a></li>
     <!-- navigation toc: --> <li><a href="#exercise-1-bias-variance-tradeoff-and-bootstrap" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Exercise 1, Bias-Variance tradeoff and Bootstrap</a></li>
     <!-- navigation toc: --> <li><a href="#part-1a-proving-the-bias-variance-tradeoff" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Part (1a) Proving the bias-variance tradeoff</a></li>
     <!-- navigation toc: --> <li><a href="#part-1b-adding-bootstrap-and-bias-variance-tradeoff" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Part (1b) Adding Bootstrap and Bias-Variance Tradeoff</a></li>
     <!-- navigation toc: --> <li><a href="#exercise-2-linear-regression-for-a-two-dimensional-function" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Exercise 2, Linear Regression for  a two-dimensional function</a></li>
     <!-- navigation toc: --> <li><a href="#2a-ordinary-least-square-on-the-franke-function-with-resampling" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;(2a) Ordinary Least Square on the Franke function  with resampling</a></li>
     <!-- navigation toc: --> <li><a href="#part-2b-resampling-techniques-adding-more-complexity" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Part (2b) Resampling techniques, adding more complexity</a></li>
     <!-- navigation toc: --> <li><a href="#part-2c-bias-variance-tradeoff" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Part (2c): Bias-variance tradeoff</a></li>
     <!-- navigation toc: --> <li><a href="#part-2d-ridge-regression-on-the-franke-function-with-resampling" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Part (2d): Ridge Regression on the Franke function  with resampling</a></li>
     <!-- navigation toc: --> <li><a href="#part-2e-lasso-regression-on-the-franke-function-with-resampling" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Part (2e): Lasso Regression on the Franke function  with resampling</a></li>

        </ul>
      </li>
    </ul>
  </div>
</div>
</div> <!-- end of navigation bar -->
<div class="container">
<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p> <!-- add vertical space -->
<a name="part0000"></a>
<!-- ------------------- main content ---------------------- -->
<div class="jumbotron">
<center>
<h1>Fourth and fifth days: Homework set 3</h1>
</center>  <!-- document title -->

<!-- author(s): Data Analysis and Machine Learning -->
<center>
<b>Data Analysis and Machine Learning</b> 
</center>
<!-- institution(s) -->
<br>
<center>
<h4>Sep 26, 2022</h4>
</center> <!-- date -->
<br>


</div> <!-- end jumbotron -->
<h2 id="day-four-and-five-exercises" class="anchor">Day four and five exercises </h2>

<p>The exercises here are somewhat longer and we expect to use at least two days on them.</p>
<h3 id="exercise-1-bias-variance-tradeoff-and-bootstrap" class="anchor">Exercise 1, Bias-Variance tradeoff and Bootstrap  </h3>

<p>This exercise is a continuation of exercise 2 from the second homework set.
In that exercise we computed the MSE-score for the training
data and the test data as functions of the complexity of a polynomial,
that is the degree of a given polynomial.
</p>

<p>One of the aims of that exercise was 
to reproduce Figure 2.11 of <a href="https://github.com/CompPhysics/MLErasmus/blob/master/doc/Textbooks/elementsstat.pdf" target="_self">Hastie et al</a>.
</p>

<p>Our data is defined by \( x\in [-3,3] \) with a total of for example \( 100 \) data points.</p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;">np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>seed()
n <span style="color: #666666">=</span> <span style="color: #666666">100</span>
maxdegree <span style="color: #666666">=</span> <span style="color: #666666">14</span>
<span style="color: #408080; font-style: italic"># Make data set.</span>
x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linspace(<span style="color: #666666">-3</span>, <span style="color: #666666">3</span>, n)<span style="color: #666666">.</span>reshape(<span style="color: #666666">-1</span>, <span style="color: #666666">1</span>)
y <span style="color: #666666">=</span> np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>x<span style="color: #666666">**2</span>) <span style="color: #666666">+</span> <span style="color: #666666">1.5</span> <span style="color: #666666">*</span> np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>(x<span style="color: #666666">-2</span>)<span style="color: #666666">**2</span>)<span style="color: #666666">+</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>normal(<span style="color: #666666">0</span>, <span style="color: #666666">0.1</span>, x<span style="color: #666666">.</span>shape)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>where \( y \) is the function we want to fit with a given polynomial.</p>
<h3 id="part-1a-proving-the-bias-variance-tradeoff" class="anchor">Part (1a) Proving the bias-variance tradeoff </h3>

<p>Consider a
dataset \( \mathcal{L} \) consisting of the data
\( \mathbf{X}_\mathcal{L}=\{(y_j, \boldsymbol{x}_j), j=0\ldots n-1\} \).
</p>

<p>Let us assume that the true data is generated from a noisy model</p>

$$
\boldsymbol{y}=f(\boldsymbol{x}) + \boldsymbol{\epsilon}.
$$

<p>Here \( \epsilon \) is normally distributed with mean zero and standard
deviation \( \sigma^2 \).
</p>

<p>In our derivation of the ordinary least squares method we defined then
an approximation to the function \( f \) in terms of the parameters
\( \boldsymbol{\beta} \) and the design matrix \( \boldsymbol{X} \) which embody our model,
that is \( \boldsymbol{\tilde{y}}=\boldsymbol{X}\boldsymbol{\beta} \).
</p>

<p>The parameters \( \boldsymbol{\beta} \) are in turn found by optimizing the means
squared error via the so-called cost function
</p>

$$
C(\boldsymbol{X},\boldsymbol{\beta}) =\frac{1}{n}\sum_{i=0}^{n-1}(y_i-\tilde{y}_i)^2=\mathbb{E}\left[(\boldsymbol{y}-\boldsymbol{\tilde{y}})^2\right].
$$

<p>Show that you can rewrite  this as</p>
$$
\mathbb{E}\left[(\boldsymbol{y}-\boldsymbol{\tilde{y}})^2\right]=\frac{1}{n}\sum_i(f_i-\mathbb{E}\left[\boldsymbol{\tilde{y}}\right])^2+\frac{1}{n}\sum_i(\tilde{y}_i-\mathbb{E}\left[\boldsymbol{\tilde{y}}\right])^2+\sigma^2.
$$

<p>Explain what the terms mean, which one is the bias and which one is
the variance and discuss their interpretations.
</p>
<h3 id="part-1b-adding-bootstrap-and-bias-variance-tradeoff" class="anchor">Part (1b) Adding Bootstrap and Bias-Variance Tradeoff </h3>

<p>Add now bootstrapping as discussed in the <a href="https://compphysics.github.io/MachineLearningECT/doc/pub/Day1/html/Day1.html" target="_self">Regression lectures</a> (scroll down to the bias-variance code).
Add also the expressions for the bias and the variance as discussed above.
</p>

<p>Discuss the bias and variance tradeoff as function
of your model complexity (the degree of the polynomial) and the number
of data points, and possibly also your training and test data.
</p>

<p>Try to make a figure similar to Fig. 2.11 of Hastie et al. You should include an analysis of the bias and variance for the test results. Figure 2.11 displays only the test and training MSEs while indicating regions of low/high bias and variance. You will most likely not get an
equally smooth curve! Note also that when you calculate the bias, in all applications you don't know the function values \( f_i \). You would hence replace them with the actual data points \( y_i \).
</p>
<h3 id="exercise-2-linear-regression-for-a-two-dimensional-function" class="anchor">Exercise 2, Linear Regression for  a two-dimensional function  </h3>

<p>This is a longer  exercise and the aim is to study in more detail various
regression methods, including the Ordinary Least Squares (OLS) method,
Ridge regression and finally Lasso regression.
The methods are in turn combined with resampling techniques.
</p>

<p>We will study how to fit polynomials to a specific
two-dimensional function called <a href="http://www.dtic.mil/dtic/tr/fulltext/u2/a081688.pdf" target="_self">Franke's
function</a>.  This
is a function which has been widely used when testing various
interpolation and fitting algorithms. Furthermore, after having
established the model and the method, we will employ resamling
like the bootstrap from the previous exercise in order to perform a
proper assessment of our models. We will also study in detail the
so-called Bias-Variance trade off.
</p>

<p>The Franke function, which is a weighted sum of four exponentials  reads as follows</p>
$$
\begin{align*}
f(x,y) &= \frac{3}{4}\exp{\left(-\frac{(9x-2)^2}{4} - \frac{(9y-2)^2}{4}\right)}+\frac{3}{4}\exp{\left(-\frac{(9x+1)^2}{49}- \frac{(9y+1)}{10}\right)} \\
&+\frac{1}{2}\exp{\left(-\frac{(9x-7)^2}{4} - \frac{(9y-3)^2}{4}\right)} -\frac{1}{5}\exp{\left(-(9x-4)^2 - (9y-7)^2\right) }.
\end{align*}
$$

<p>The function will be defined for \( x,y\in [0,1] \).  Our first step will
be to perform an OLS regression analysis of this function, trying out
a polynomial fit with an \( x \) and \( y \) dependence of the form \( [x, y,
x^2, y^2, xy, \dots] \). We will also include cross-validation (or bootstrap) as
resampling technique.  As in homeworks 1 and 2, we can use a uniform
distribution to set up the arrays of values for \( x \) and \( y \), or as in
the example below just a set of fixed 
values for \( x \) and \( y \) with a given step
size.  We will fit a
function (for example a polynomial) of \( x \) and \( y \).  Thereafter we
will repeat much of the same procedure using the Ridge and Lasso
regression methods, introducing thus a dependence on the bias
(penalty) \( \lambda \).
</p>

<p>Finally we are going to use (real) digital terrain data and try to
reproduce these data using the same methods. We will also try to go
beyond the second-order polynomials metioned above and explore 
which polynomial fits the data best.
</p>

<p>The Python fucntion for the Franke function is included here (it performs also a three-dimensional plot of it)</p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">mpl_toolkits.mplot3d</span> <span style="color: #008000; font-weight: bold">import</span> Axes3D
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">matplotlib</span> <span style="color: #008000; font-weight: bold">import</span> cm
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">matplotlib.ticker</span> <span style="color: #008000; font-weight: bold">import</span> LinearLocator, FormatStrFormatter
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">random</span> <span style="color: #008000; font-weight: bold">import</span> random, seed

fig <span style="color: #666666">=</span> plt<span style="color: #666666">.</span>figure()
ax <span style="color: #666666">=</span> fig<span style="color: #666666">.</span>gca(projection<span style="color: #666666">=</span><span style="color: #BA2121">&#39;3d&#39;</span>)

<span style="color: #408080; font-style: italic"># Make data.</span>
x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>arange(<span style="color: #666666">0</span>, <span style="color: #666666">1</span>, <span style="color: #666666">0.05</span>)
y <span style="color: #666666">=</span> np<span style="color: #666666">.</span>arange(<span style="color: #666666">0</span>, <span style="color: #666666">1</span>, <span style="color: #666666">0.05</span>)
x, y <span style="color: #666666">=</span> np<span style="color: #666666">.</span>meshgrid(x,y)


<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">FrankeFunction</span>(x,y):
    term1 <span style="color: #666666">=</span> <span style="color: #666666">0.75*</span>np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>(<span style="color: #666666">0.25*</span>(<span style="color: #666666">9*</span>x<span style="color: #666666">-2</span>)<span style="color: #666666">**2</span>) <span style="color: #666666">-</span> <span style="color: #666666">0.25*</span>((<span style="color: #666666">9*</span>y<span style="color: #666666">-2</span>)<span style="color: #666666">**2</span>))
    term2 <span style="color: #666666">=</span> <span style="color: #666666">0.75*</span>np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>((<span style="color: #666666">9*</span>x<span style="color: #666666">+1</span>)<span style="color: #666666">**2</span>)<span style="color: #666666">/49.0</span> <span style="color: #666666">-</span> <span style="color: #666666">0.1*</span>(<span style="color: #666666">9*</span>y<span style="color: #666666">+1</span>))
    term3 <span style="color: #666666">=</span> <span style="color: #666666">0.5*</span>np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>(<span style="color: #666666">9*</span>x<span style="color: #666666">-7</span>)<span style="color: #666666">**2/4.0</span> <span style="color: #666666">-</span> <span style="color: #666666">0.25*</span>((<span style="color: #666666">9*</span>y<span style="color: #666666">-3</span>)<span style="color: #666666">**2</span>))
    term4 <span style="color: #666666">=</span> <span style="color: #666666">-0.2*</span>np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>(<span style="color: #666666">9*</span>x<span style="color: #666666">-4</span>)<span style="color: #666666">**2</span> <span style="color: #666666">-</span> (<span style="color: #666666">9*</span>y<span style="color: #666666">-7</span>)<span style="color: #666666">**2</span>)
    <span style="color: #008000; font-weight: bold">return</span> term1 <span style="color: #666666">+</span> term2 <span style="color: #666666">+</span> term3 <span style="color: #666666">+</span> term4


z <span style="color: #666666">=</span> FrankeFunction(x, y)

<span style="color: #408080; font-style: italic"># Plot the surface.</span>
surf <span style="color: #666666">=</span> ax<span style="color: #666666">.</span>plot_surface(x, y, z, cmap<span style="color: #666666">=</span>cm<span style="color: #666666">.</span>coolwarm,
                       linewidth<span style="color: #666666">=0</span>, antialiased<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>)

<span style="color: #408080; font-style: italic"># Customize the z axis.</span>
ax<span style="color: #666666">.</span>set_zlim(<span style="color: #666666">-0.10</span>, <span style="color: #666666">1.40</span>)
ax<span style="color: #666666">.</span>zaxis<span style="color: #666666">.</span>set_major_locator(LinearLocator(<span style="color: #666666">10</span>))
ax<span style="color: #666666">.</span>zaxis<span style="color: #666666">.</span>set_major_formatter(FormatStrFormatter(<span style="color: #BA2121">&#39;</span><span style="color: #BB6688; font-weight: bold">%.02f</span><span style="color: #BA2121">&#39;</span>))

<span style="color: #408080; font-style: italic"># Add a color bar which maps values to colors.</span>
fig<span style="color: #666666">.</span>colorbar(surf, shrink<span style="color: #666666">=0.5</span>, aspect<span style="color: #666666">=5</span>)

plt<span style="color: #666666">.</span>show()
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>
<h3 id="2a-ordinary-least-square-on-the-franke-function-with-resampling" class="anchor">(2a) Ordinary Least Square on the Franke function  with resampling </h3>

<p>We will generate our own dataset for a function
\( \mathrm{FrankeFunction}(x,y) \) with \( x,y \in [0,1] \). The function
\( f(x,y) \) is the Franke function. You should explore also the addition
an added stochastic noise to this function using the normal
distribution \( \cal{N}(0,1) \).
</p>

<p>Write your own code (using either a matrix inversion or a singular
value decomposition from e.g., <b>numpy</b> ) or use your code from
homeworks 1 and 2 and perform a standard least square regression
analysis using polynomials in \( x \) and \( y \) up to fifth order. You can use <b>scikit-learn</b> as well.
</p>

<p>Evaluate the Mean Squared error (MSE)</p>

$$ MSE(\hat{y},\hat{\tilde{y}}) = \frac{1}{n}
\sum_{i=0}^{n-1}(y_i-\tilde{y}_i)^2, 
$$

<p>and the \( R^2 \) score function.  If \( \tilde{\hat{y}}_i \) is the predicted
value of the \( i-th \) sample and \( y_i \) is the corresponding true value,
then the score \( R^2 \) is defined as
</p>

$$
R^2(\hat{y}, \tilde{\hat{y}}) = 1 - \frac{\sum_{i=0}^{n - 1} (y_i - \tilde{y}_i)^2}{\sum_{i=0}^{n - 1} (y_i - \bar{y})^2},
$$

<p>where we have defined the mean value  of \( \hat{y} \) as</p>

$$
\bar{y} =  \frac{1}{n} \sum_{i=0}^{n - 1} y_i.
$$

<p>To set up the design matrix, the following code can be used</p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">FrankeFunction</span>(x,y):
	term1 <span style="color: #666666">=</span> <span style="color: #666666">0.75*</span>np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>(<span style="color: #666666">0.25*</span>(<span style="color: #666666">9*</span>x<span style="color: #666666">-2</span>)<span style="color: #666666">**2</span>) <span style="color: #666666">-</span> <span style="color: #666666">0.25*</span>((<span style="color: #666666">9*</span>y<span style="color: #666666">-2</span>)<span style="color: #666666">**2</span>))
	term2 <span style="color: #666666">=</span> <span style="color: #666666">0.75*</span>np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>((<span style="color: #666666">9*</span>x<span style="color: #666666">+1</span>)<span style="color: #666666">**2</span>)<span style="color: #666666">/49.0</span> <span style="color: #666666">-</span> <span style="color: #666666">0.1*</span>(<span style="color: #666666">9*</span>y<span style="color: #666666">+1</span>))
	term3 <span style="color: #666666">=</span> <span style="color: #666666">0.5*</span>np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>(<span style="color: #666666">9*</span>x<span style="color: #666666">-7</span>)<span style="color: #666666">**2/4.0</span> <span style="color: #666666">-</span> <span style="color: #666666">0.25*</span>((<span style="color: #666666">9*</span>y<span style="color: #666666">-3</span>)<span style="color: #666666">**2</span>))
	term4 <span style="color: #666666">=</span> <span style="color: #666666">-0.2*</span>np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>(<span style="color: #666666">9*</span>x<span style="color: #666666">-4</span>)<span style="color: #666666">**2</span> <span style="color: #666666">-</span> (<span style="color: #666666">9*</span>y<span style="color: #666666">-7</span>)<span style="color: #666666">**2</span>)
	<span style="color: #008000; font-weight: bold">return</span> term1 <span style="color: #666666">+</span> term2 <span style="color: #666666">+</span> term3 <span style="color: #666666">+</span> term4


<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">create_X</span>(x, y, n ):
	<span style="color: #008000; font-weight: bold">if</span> <span style="color: #008000">len</span>(x<span style="color: #666666">.</span>shape) <span style="color: #666666">&gt;</span> <span style="color: #666666">1</span>:
		x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>ravel(x)
		y <span style="color: #666666">=</span> np<span style="color: #666666">.</span>ravel(y)

	N <span style="color: #666666">=</span> <span style="color: #008000">len</span>(x)
	l <span style="color: #666666">=</span> <span style="color: #008000">int</span>((n<span style="color: #666666">+1</span>)<span style="color: #666666">*</span>(n<span style="color: #666666">+2</span>)<span style="color: #666666">/2</span>)		<span style="color: #408080; font-style: italic"># Number of elements in beta</span>
	X <span style="color: #666666">=</span> np<span style="color: #666666">.</span>ones((N,l))

	<span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(<span style="color: #666666">1</span>,n<span style="color: #666666">+1</span>):
		q <span style="color: #666666">=</span> <span style="color: #008000">int</span>((i)<span style="color: #666666">*</span>(i<span style="color: #666666">+1</span>)<span style="color: #666666">/2</span>)
		<span style="color: #008000; font-weight: bold">for</span> k <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(i<span style="color: #666666">+1</span>):
			X[:,q<span style="color: #666666">+</span>k] <span style="color: #666666">=</span> (x<span style="color: #666666">**</span>(i<span style="color: #666666">-</span>k))<span style="color: #666666">*</span>(y<span style="color: #666666">**</span>k)

	<span style="color: #008000; font-weight: bold">return</span> X


<span style="color: #408080; font-style: italic"># Making meshgrid of datapoints and compute Franke&#39;s function</span>
n <span style="color: #666666">=</span> <span style="color: #666666">5</span>
N <span style="color: #666666">=</span> <span style="color: #666666">1000</span>
x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>sort(np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>uniform(<span style="color: #666666">0</span>, <span style="color: #666666">1</span>, N))
y <span style="color: #666666">=</span> np<span style="color: #666666">.</span>sort(np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>uniform(<span style="color: #666666">0</span>, <span style="color: #666666">1</span>, N))
z <span style="color: #666666">=</span> FrankeFunction(x, y)
X <span style="color: #666666">=</span> create_X(x, y, n<span style="color: #666666">=</span>n)    
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>
<h3 id="part-2b-resampling-techniques-adding-more-complexity" class="anchor">Part (2b) Resampling techniques, adding more complexity </h3>

<p>Perform a resampling of the data where you split the data in training
data and test data. Here you can write your own function or use the
function for splitting training data provided by <b>Scikit-Learn</b>.
This function is called \( train\_test\_split \).   You should also renormalize your data.
</p>

<p>It is normal in essentially all Machine Learning studies to split the
data in a training set and a test set (sometimes also an additional
validation set).  There
is no explicit recipe for how much data should be included as training
data and say test data.  An accepted rule of thumb is to use
approximately \( 2/3 \) to \( 4/5 \) of the data as training data.
</p>

<p>Use then the _bootstrap code you developed in the previous exercise to resample your data
and evaluate again the MSE function resulting
from the test data. 
</p>
<h3 id="part-2c-bias-variance-tradeoff" class="anchor">Part (2c): Bias-variance tradeoff </h3>

<p>With a code which does OLS and includes bootstrap
we will now discuss the bias-variance tradeoff in the context of
continuous predictions such as regression. However, many of the
intuitions and ideas discussed here also carry over to classification
tasks and basically all Machine Learning algorithms. 
</p>

<p>Use the code from exercise 1 above and implement the bootstrap
resampling and perform a bias-variance tradeoff analysis like you did
in exercise 1.
</p>
<h3 id="part-2d-ridge-regression-on-the-franke-function-with-resampling" class="anchor">Part (2d): Ridge Regression on the Franke function  with resampling </h3>

<p>Write your own code for the Ridge method, either using matrix
inversion or the singular value decomposition ir use <b>scikit-learn</b> 
Perform the same analysis as in the
previous three steps (for the same polynomials and include resampling
techniques) but now for different values of \( \lambda \). Compare and
analyze your results with those obtained in parts 2a-2c). Study the
dependence on \( \lambda \).
</p>

<p>Study also the bias-variance tradeoff as function of various values of
the parameter \( \lambda \). Comment your results. 
</p>
<h3 id="part-2e-lasso-regression-on-the-franke-function-with-resampling" class="anchor">Part (2e): Lasso Regression on the Franke function  with resampling </h3>

<p>This part is essentially a repeat of the previous ones, but now
with Lasso regression. Write either your own code or
use the functionalities of <b>Scikit-Learn</b> (recommended). 
Give a
critical discussion of the three methods and a judgement of which
model fits the data best.
</p>

<p>
<!-- navigation buttons at the bottom of the page -->
<ul class="pagination">
  <li class="active"><a href="._hw3-bs000.html">1</a></li>
</ul>
<!-- ------------------- end of main content --------------- -->
</div>  <!-- end container -->
<!-- include javascript, jQuery *first* -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>
<!-- Bootstrap footer
<footer>
<a href="https://..."><img width="250" align=right src="https://..."></a>
</footer>
-->
</body>
</html>

